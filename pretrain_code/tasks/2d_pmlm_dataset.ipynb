{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlm import *\n",
    "from fairseq import checkpoint_utils, options, tasks, utils, data, options\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.mlm import *\n",
    "sys.path.append(\"..\")\n",
    "from criterions.mlm import *\n",
    "from fairseq.data import (\n",
    "    data_utils,\n",
    "    Dictionary,\n",
    "    iterators,\n",
    "    FairseqDataset,\n",
    "    IdDataset,\n",
    "    NestedDictionaryDataset,\n",
    "    NumSamplesDataset,\n",
    "    NumelDataset,\n",
    "    PrependTokenDataset,\n",
    "    SortDataset,\n",
    "    TokenBlockDataset,\n",
    "    MaskTokensDataset,\n",
    "    PadDataset,\n",
    "    BaseWrapperDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_pretrained_model = '../../../../../base_model_checkpoint/checkpoint100.pt'\n",
    "arg_overrides = { \"data\": '../../../../../pretrain_data/rnacentral_data_cdhit_ftvocab_bin' }\n",
    "\n",
    "models, args, task = checkpoint_utils.load_model_ensemble_and_task(mlm_pretrained_model.split(os.pathsep), \n",
    "                                                                   arg_overrides=arg_overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "split = 'valid'\n",
    "paths = utils.split_paths(task.args.data)\n",
    "assert len(paths) > 0\n",
    "data_path = paths[(epoch - 1) % len(paths)]\n",
    "split_path = os.path.join(data_path, split)\n",
    "dataset = data_utils.load_indexed_dataset(\n",
    "            split_path,\n",
    "            task.source_dictionary,\n",
    "            task.args.dataset_impl,\n",
    "            combine=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenBlockDataset(\n",
    "            dataset,\n",
    "            dataset.sizes,\n",
    "            task.args.tokens_per_sample - 1,  # one less for <s>\n",
    "            pad=task.source_dictionary.pad(),\n",
    "            eos=task.source_dictionary.eos(),\n",
    "            break_mode=task.args.sample_break_mode,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrependTokenDataset(dataset, task.source_dictionary.bos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'rna-pretrain' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n rna-pretrain ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import Dictionary, data_utils, BaseWrapperDataset, LRUCacheDataset\n",
    "from functools import lru_cache\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian(x):\n",
    "    return math.exp(-0.5*(x*x))\n",
    "\n",
    "def paired(x,y,lamda=0.8):\n",
    "    if x == 5 and y == 6:\n",
    "        return 2\n",
    "    elif x == 4 and y == 7:\n",
    "        return 3\n",
    "    elif x == 4 and y == 6:\n",
    "        return lamda\n",
    "    elif x == 6 and y == 5:\n",
    "        return 2\n",
    "    elif x == 7 and y == 4:\n",
    "        return 3\n",
    "    elif x == 6 and y == 4:\n",
    "        return lamda\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def creatmat(data):\n",
    "    mat = np.zeros([len(data),len(data)])\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            coefficient = 0\n",
    "            for add in range(30):\n",
    "                if i - add >= 0 and j + add <len(data):\n",
    "                    score = paired(data[i - add],data[j + add])\n",
    "                    if score == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        coefficient = coefficient + score * Gaussian(add)\n",
    "                else:\n",
    "                    break\n",
    "            if coefficient > 0:\n",
    "                for add in range(1,30):\n",
    "                    if i + add < len(data) and j - add >= 0:\n",
    "                        score = paired(data[i + add],data[j - add])\n",
    "                        if score == 0:\n",
    "                            break\n",
    "                        else:\n",
    "                            coefficient = coefficient + score * Gaussian(add)\n",
    "                    else:\n",
    "                        break\n",
    "            mat[[i],[j]] = coefficient\n",
    "    return mat\n",
    "\n",
    "base_range_lst = [1]\n",
    "lamda_lst = [0.7,0.8,0.9]\n",
    "\n",
    "def creatmat_new(data, base_range=30, lamda=0.8):\n",
    "    paird_map = np.array([[paired(i,j,lamda) for i in range(30)] for j in range(30)])\n",
    "    data_index = np.arange(0,len(data))\n",
    "    # np.indices((2,2))    \n",
    "    coefficient = np.zeros([len(data),len(data)])\n",
    "    # mat = np.zeros((len(data),len(data)))\n",
    "    score_mask = np.full((len(data),len(data)),True)\n",
    "    for add in range(base_range):\n",
    "        data_index_x = data_index - add\n",
    "        data_index_y = data_index + add\n",
    "        score_mask = ((data_index_x >= 0)[:,None] & (data_index_y < len(data))[None,:]) & score_mask\n",
    "        data_index_x,data_index_y = np.meshgrid(data_index_x.clip(0,len(data) - 1),data_index_y.clip(0,len(data) - 1),indexing='ij')\n",
    "        score = paird_map[data[data_index_x],data[data_index_y]]\n",
    "        score_mask = score_mask & (score != 0)\n",
    "        \n",
    "        coefficient = coefficient + score * score_mask * Gaussian(add)\n",
    "        if ~(score_mask.any()) :\n",
    "            break\n",
    "    score_mask = coefficient > 0\n",
    "    for add in range(1,base_range):\n",
    "        data_index_x = data_index + add\n",
    "        data_index_y = data_index - add\n",
    "        score_mask = ((data_index_x < len(data))[:,None] & (data_index_y >= 0)[None,:]) & score_mask\n",
    "        data_index_x,data_index_y = np.meshgrid(data_index_x.clip(0,len(data) - 1),data_index_y.clip(0,len(data) - 1),indexing='ij')\n",
    "        score = paird_map[data[data_index_x],data[data_index_y]]\n",
    "        score_mask = score_mask & (score != 0)\n",
    "        \n",
    "        coefficient = coefficient + score * score_mask * Gaussian(add)\n",
    "        if ~(score_mask.any()) :\n",
    "            break\n",
    "    return coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskTokensDataset(BaseWrapperDataset):\n",
    "    \"\"\"\n",
    "    A wrapper Dataset for masked language modeling.\n",
    "    Input items are masked according to the specified masking probability.\n",
    "    Args:\n",
    "        dataset: Dataset to wrap.\n",
    "        sizes: Sentence lengths\n",
    "        vocab: Dictionary with the vocabulary and special tokens.\n",
    "        pad_idx: Id of pad token in vocab\n",
    "        mask_idx: Id of mask token in vocab\n",
    "        return_masked_tokens: controls whether to return the non-masked tokens\n",
    "            (the default) or to return a tensor with the original masked token\n",
    "            IDs (and *pad_idx* elsewhere). The latter is useful as targets for\n",
    "            masked LM training.\n",
    "        seed: Seed for random number generator for reproducibility.\n",
    "        mask_prob: probability of replacing a token with *mask_idx*.\n",
    "        leave_unmasked_prob: probability that a masked token is unmasked.\n",
    "        random_token_prob: probability of replacing a masked token with a\n",
    "            random token from the vocabulary.\n",
    "        freq_weighted_replacement: sample random replacement words based on\n",
    "            word frequencies in the vocab.\n",
    "        mask_whole_words: only mask whole words. This should be a byte mask\n",
    "            over vocab indices, indicating whether it is the beginning of a\n",
    "            word. We will extend any mask to encompass the whole word.\n",
    "        bpe: BPE to use for whole-word masking.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def apply_mask(cls, dataset: torch.utils.data.Dataset, *args, **kwargs):\n",
    "        \"\"\"Return the source and target datasets for masked LM training.\"\"\"\n",
    "        dataset = LRUCacheDataset(dataset)\n",
    "        return (\n",
    "            LRUCacheDataset(cls(dataset, *args, **kwargs, return_masked_tokens=False)),\n",
    "            LRUCacheDataset(cls(dataset, *args, **kwargs, return_masked_tokens=True)),\n",
    "            LRUCacheDataset(cls(dataset, *args, **kwargs, two_dim_score=True, two_dim_mask=-1)),\n",
    "        )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: torch.utils.data.Dataset,\n",
    "        vocab: Dictionary,\n",
    "        pad_idx: int,\n",
    "        mask_idx: int,\n",
    "        return_masked_tokens: bool = False,\n",
    "        seed: int = 1,\n",
    "        mask_prob: float = 0.15,\n",
    "        leave_unmasked_prob: float = 0.1,\n",
    "        random_token_prob: float = 0.1,\n",
    "        freq_weighted_replacement: bool = False,\n",
    "        two_dim_score: bool = False,\n",
    "        two_dim_mask: int = -1,\n",
    "        mask_whole_words: torch.Tensor = None,\n",
    "    ):\n",
    "        assert 0.0 < mask_prob < 1.0\n",
    "        assert 0.0 <= random_token_prob <= 1.0\n",
    "        assert 0.0 <= leave_unmasked_prob <= 1.0\n",
    "        assert random_token_prob + leave_unmasked_prob <= 1.0\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.vocab = vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.mask_idx = mask_idx\n",
    "        self.return_masked_tokens = return_masked_tokens\n",
    "        self.seed = seed\n",
    "        self.mask_prob = mask_prob\n",
    "        self.leave_unmasked_prob = leave_unmasked_prob\n",
    "        self.random_token_prob = random_token_prob\n",
    "        self.two_dim_score = two_dim_score\n",
    "        self.two_dim_mask = two_dim_mask\n",
    "        self.mask_whole_words = mask_whole_words\n",
    "\n",
    "        if random_token_prob > 0.0:\n",
    "            if freq_weighted_replacement:\n",
    "                weights = np.array(self.vocab.count)\n",
    "            else:\n",
    "                weights = np.ones(len(self.vocab))\n",
    "            weights[: self.vocab.nspecial] = 0\n",
    "            self.weights = weights / weights.sum()\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "    @property\n",
    "    def can_reuse_epoch_itr_across_epochs(self):\n",
    "        return True  # only the noise changes, not item sizes\n",
    "\n",
    "    def set_epoch(self, epoch, **unused):\n",
    "        super().set_epoch(epoch)\n",
    "        self.epoch = epoch\n",
    "\n",
    "    @lru_cache(maxsize=8)\n",
    "    def __getitem__(self, index: int):\n",
    "        with data_utils.numpy_seed(self.seed, self.epoch, index):\n",
    "            item = self.dataset[index]\n",
    "            sz = len(item)\n",
    "\n",
    "            assert (\n",
    "                self.mask_idx not in item\n",
    "            ), \"Dataset contains mask_idx (={}), this is not expected!\".format(\n",
    "                self.mask_idx,\n",
    "            )\n",
    "\n",
    "            if self.mask_whole_words is not None:\n",
    "                word_begins_mask = self.mask_whole_words.gather(0, item)\n",
    "                word_begins_idx = word_begins_mask.nonzero().view(-1)\n",
    "                sz = len(word_begins_idx)\n",
    "                words = np.split(word_begins_mask, word_begins_idx)[1:]\n",
    "                assert len(words) == sz\n",
    "                word_lens = list(map(len, words))\n",
    "\n",
    "            # decide elements to mask\n",
    "            mask = np.full(sz, False)\n",
    "            num_mask = int(\n",
    "                # add a random number for probabilistic rounding\n",
    "                self.mask_prob * sz\n",
    "                + np.random.rand()\n",
    "            )\n",
    "            mask[np.random.choice(sz, num_mask, replace=False)] = True\n",
    "\n",
    "            # return 2d-dim socre:\n",
    "            if self.two_dim_score:\n",
    "                item_len = len(item.numpy())\n",
    "                two_dim_matrix = np.zeros((len(base_range_lst)*len(lamda_lst),item_len,item_len))\n",
    "                padding_dim = 0\n",
    "                for base_range in base_range_lst:\n",
    "                    for lamda in lamda_lst:\n",
    "                        new_matrix = creatmat_new(item.numpy(),base_range,lamda)\n",
    "                        new_matrix[mask,:] = -1\n",
    "                        new_matrix[:,mask] = -1\n",
    "                        two_dim_matrix[padding_dim,:,:] = new_matrix\n",
    "                        padding_dim += 1\n",
    "                # use -1 represent mask\n",
    "                # matrix[mask,:] = self.two_dim_mask\n",
    "                # matrix[:,mask] = self.two_dim_mask\n",
    "                # print(two_dim_matrix.shape)\n",
    "                return torch.from_numpy(two_dim_matrix)\n",
    "            \n",
    "            # return target\n",
    "            if self.return_masked_tokens:\n",
    "                # exit early if we're just returning the masked tokens\n",
    "                # (i.e., the targets for masked LM training)\n",
    "                if self.mask_whole_words is not None:\n",
    "                    mask = np.repeat(mask, word_lens)\n",
    "                new_item = np.full(len(mask), self.pad_idx)\n",
    "                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8)) == 1]\n",
    "                return torch.from_numpy(new_item)\n",
    "\n",
    "            # decide unmasking and random replacement\n",
    "            rand_or_unmask_prob = self.random_token_prob + self.leave_unmasked_prob\n",
    "            if rand_or_unmask_prob > 0.0:\n",
    "                rand_or_unmask = mask & (np.random.rand(sz) < rand_or_unmask_prob)\n",
    "                if self.random_token_prob == 0.0:\n",
    "                    unmask = rand_or_unmask\n",
    "                    rand_mask = None\n",
    "                elif self.leave_unmasked_prob == 0.0:\n",
    "                    unmask = None\n",
    "                    rand_mask = rand_or_unmask\n",
    "                else:\n",
    "                    unmask_prob = self.leave_unmasked_prob / rand_or_unmask_prob\n",
    "                    decision = np.random.rand(sz) < unmask_prob\n",
    "                    unmask = rand_or_unmask & decision\n",
    "                    rand_mask = rand_or_unmask & (~decision)\n",
    "            else:\n",
    "                unmask = rand_mask = None\n",
    "\n",
    "            if unmask is not None:\n",
    "                mask = mask ^ unmask\n",
    "\n",
    "            if self.mask_whole_words is not None:\n",
    "                mask = np.repeat(mask, word_lens)\n",
    "\n",
    "            new_item = np.copy(item)\n",
    "            new_item[mask] = self.mask_idx\n",
    "            if rand_mask is not None:\n",
    "                num_rand = rand_mask.sum()\n",
    "                if num_rand > 0:\n",
    "                    if self.mask_whole_words is not None:\n",
    "                        rand_mask = np.repeat(rand_mask, word_lens)\n",
    "                        num_rand = rand_mask.sum()\n",
    "\n",
    "                    new_item[rand_mask] = np.random.choice(\n",
    "                        len(self.vocab),\n",
    "                        num_rand,\n",
    "                        p=self.weights,\n",
    "                    )\n",
    "\n",
    "            return torch.from_numpy(new_item)\n",
    "        \n",
    "src_dataset, tgt_dataset, twod_dataset = MaskTokensDataset.apply_mask(\n",
    "            dataset,\n",
    "            task.source_dictionary,\n",
    "            pad_idx=task.source_dictionary.pad(),\n",
    "            mask_idx=task.mask_idx,\n",
    "            seed=task.args.seed,\n",
    "            mask_prob=task.args.mask_prob,\n",
    "            leave_unmasked_prob=task.args.leave_unmasked_prob,\n",
    "            random_token_prob=task.args.random_token_prob,\n",
    "            freq_weighted_replacement=task.args.freq_weighted_replacement,\n",
    "            mask_whole_words=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],\n",
    "              [4,5]])\n",
    "\n",
    "b = np.array([[11,22,33],\n",
    "              [44,55,66],\n",
    "              [77,88,99]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twod_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_tokens(\n",
    "    values,\n",
    "    pad_idx,\n",
    "    eos_idx=None,\n",
    "    left_pad=False,\n",
    "    move_eos_to_beginning=False,\n",
    "    pad_to_length=None,\n",
    "    pad_to_multiple=1,\n",
    "):\n",
    "    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n",
    "    size = max(v.size(0) for v in values)\n",
    "    size = size if pad_to_length is None else max(size, pad_to_length)\n",
    "    if pad_to_multiple != 1 and size % pad_to_multiple != 0:\n",
    "        size = int(((size - 0.1) // pad_to_multiple + 1) * pad_to_multiple)\n",
    "    res = values[0].new(len(values), size).fill_(pad_idx)\n",
    "\n",
    "    def copy_tensor(src, dst):\n",
    "        assert dst.numel() == src.numel()\n",
    "        if move_eos_to_beginning:\n",
    "            if eos_idx is None:\n",
    "                # if no eos_idx is specified, then use the last token in src\n",
    "                dst[0] = src[-1]\n",
    "            else:\n",
    "                dst[0] = eos_idx\n",
    "            dst[1:] = src[:-1]\n",
    "        else:\n",
    "            dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        copy_tensor(v, res[i][size - len(v) :] if left_pad else res[i][: len(v)])\n",
    "    return res\n",
    "\n",
    "\n",
    "samples = [src_dataset[0],src_dataset[1],src_dataset[2],src_dataset[3],src_dataset[4],src_dataset[5]]\n",
    "oned_input = collate_tokens(samples,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class PadDataset_2d(BaseWrapperDataset):\n",
    "    def __init__(self, dataset, pad_idx, left_pad):\n",
    "        super().__init__(dataset)\n",
    "        self.pad_idx = pad_idx\n",
    "        self.left_pad = left_pad\n",
    "\n",
    "    def collater(self, samples):\n",
    "        return collate_tokens_2d(samples, self.pad_idx, left_pad=self.left_pad)\n",
    "\n",
    "def collate_tokens_2d(\n",
    "    values,\n",
    "    pad_idx,\n",
    "    eos_idx=None,\n",
    "    left_pad=False,\n",
    "    move_eos_to_beginning=False,\n",
    "    pad_to_length=None,\n",
    "    pad_to_multiple=1,\n",
    "):\n",
    "    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n",
    "    size = max(v.size(1) for v in values)\n",
    "    size = size if pad_to_length is None else max(size, pad_to_length)\n",
    "    diverse_dim = values[0].size(0)\n",
    "    if pad_to_multiple != 1 and size % pad_to_multiple != 0:\n",
    "        size = int(((size - 0.1) // pad_to_multiple + 1) * pad_to_multiple)\n",
    "    res = np.full(shape=(len(values),diverse_dim,size,size),fill_value=pad_idx).astype(float)\n",
    "\n",
    "    # def copy_tensor(src, dst):\n",
    "    #     assert dst.numel() == src.numel()\n",
    "    #     if move_eos_to_beginning:\n",
    "    #         if eos_idx is None:\n",
    "    #             # if no eos_idx is specified, then use the last token in src\n",
    "    #             dst[0] = src[-1]\n",
    "    #         else:\n",
    "    #             dst[0] = eos_idx\n",
    "    #         dst[1:] = src[:-1]\n",
    "    #     else:\n",
    "    #         dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        # copy_tensor(v, res[i][size - len(v) :, size - len(v) :] if left_pad else res[i][:len(v),:len(v)])\n",
    "        res[i][:,:v.size(1),:v.size(1)] = v.numpy()\n",
    "    return torch.from_numpy(res)\n",
    "\n",
    "samples = [twod_dataset[0],twod_dataset[1]]\n",
    "twod_input = collate_tokens_2d(samples,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  3.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  3.,\n",
       "         0.,  0.,  0.,  3.,  0.,  3.,  0., -1.,  3., -1.,  3.,  0.,  3.,  0.,\n",
       "        -1., -1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        -1.,  3.,  3.,  0.,  0.,  3.,  0.,  0.,  0., -1., -1.,  0., -1., -1.,\n",
       "         3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "        -1.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
       "         0.,  0., -1.,  0.,  3., -1.,  0., -1.,  0.,  3.,  0.,  0.,  0.,  0.,\n",
       "        -1.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  3.,  0.,\n",
       "         0., -1.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0., -1.,  3.,  0.,  3.,  0.,  0.,  0., -1., -1.,  0.,  0.,  3.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  3.,  0.,  0.,  0.,  3.,  0.,\n",
       "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  3.,  0.,  0., -1.,  0.,\n",
       "        -1.,  0.,  3.,  0., -1.,  0.,  0.,  0.,  0.,  3.,  0.,  0., -1., -1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  3.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         0.,  3., -1.,  0.,  0.,  0.,  0.,  0.,  3., -1., -1., -1.,  3.,  3.,\n",
       "        -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  3.,  0.,\n",
       "         0.,  3.,  0.,  0., -1.,  0.,  3.,  0.,  0.,  3.,  0.,  3.,  0.,  0.,\n",
       "         0.,  0.,  3.,  3.,  3.,  3.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,\n",
       "         0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  3., -1.,  0.,  0.,  0.,  3.,\n",
       "         3.,  0.,  3.,  3.,  0.,  0.,  0., -1.,  0.,  3., -1.,  0.,  0., -1.,\n",
       "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         0.,  0., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  3.,  3.,  0.,  0.,\n",
       "        -1., -1., -1.,  0.,  3.,  0.,  3.,  3.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "        -1.,  0.,  0.,  3.,  3.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  3.,  0.,\n",
       "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  3., -1.,  0.,  0.,  3.,  0.,\n",
       "         0.,  0.,  0.,  0., -1.,  0.,  3.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
       "         3., -1.,  0.,  3.,  0., -1.,  0.,  3.,  0., -1.,  0.,  3.,  0.,  0.,\n",
       "         0., -1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         3.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
       "         3.,  3.,  0.,  0.,  3.,  0.,  0.,  3., -1., -1., -1.,  0.,  3.,  0.,\n",
       "         3.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,\n",
       "         0.,  3.,  0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  3.,  0., -1.,  0.,\n",
       "         0.,  3.,  0.,  0.,  3.,  3.,  0.,  0.,  0.,  0.,  0.,  3., -1.,  0.,\n",
       "         3.,  0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  3.,  0.,  3.,  0.,\n",
       "         0.,  3., -1.,  0.,  0.,  0.,  0., -1., -1., -1., -1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  3.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0.,  3.,  3.,  3.,\n",
       "         0.,  3.,  0., -1.,  0.,  0.,  0.,  0.,  3., -1.,  0.,  0.,  0., -1.,\n",
       "        -1.,  3.,  0.,  0.,  3., -1.,  0., -1.,  3.,  0.,  3.,  0.,  3.,  0.,\n",
       "         0.,  0.,  3.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  3.,  0.,  0.,  0.,\n",
       "         3., -1.,  0.,  3.,  0.,  0.,  0.,  3.,  3.,  0., -1.,  0., -1.,  3.,\n",
       "         0.,  0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twod_input[0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 660, 660])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twod_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "        0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 3, 0, 2, 0, 2, 0, 0, 3, 3, 0, 0,\n",
       "        0, 0, 0, 0, 2, 0, 0, 0, 3, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "        0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 2, 0, 0, 0, 0, 0, 3, 4, 3, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2,\n",
       "        0, 2, 0, 3, 4, 4, 4, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 2, 0, 2, 0, 0,\n",
       "        2, 0, 0, 0, 3, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 2,\n",
       "        0, 0, 2, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        4, 3, 0, 0, 0, 0, 2, 0, 0, 0, 3, 4, 4, 3, 0, 3, 4, 3, 0, 0, 2, 0, 3, 3,\n",
       "        0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 3, 3, 0, 3, 4, 3, 0, 0, 2, 0, 3, 3, 0,\n",
       "        0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 3, 4, 4, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3,\n",
       "        4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 3, 3, 0, 0, 0, 2, 0, 2,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twod_input[1,0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twod_dataset[0].new(6, 100, 100).fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.full(shape=(6,10),fill_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1129, 1129])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [twod_dataset[0],twod_dataset[1],twod_dataset[2]]\n",
    "collate_tokens_2d(samples,1,False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "netinput_dataset = {\n",
    "        'id': IdDataset(),\n",
    "        'net_input': {\n",
    "            'src_tokens': PadDataset(\n",
    "                src_dataset,\n",
    "                pad_idx=task.source_dictionary.pad(),\n",
    "                left_pad=False,\n",
    "            ),\n",
    "            'src_lengths': NumelDataset(src_dataset, reduce=False),\n",
    "            'pair_tokens':PadDataset_2d(\n",
    "                twod_dataset,\n",
    "                pad_idx=0,\n",
    "                left_pad=False,\n",
    "            ),\n",
    "        },\n",
    "        'target': PadDataset(\n",
    "            tgt_dataset,\n",
    "            pad_idx=task.source_dictionary.pad(),\n",
    "            left_pad=False,\n",
    "        ),\n",
    "        'nsentences': NumSamplesDataset(),\n",
    "        'ntokens': NumelDataset(src_dataset, reduce=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "c = 3\n",
    "torch.Tensor([2,3,4])[:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3536, 0.1432, 1.0000, 1.0000],\n",
       "         [0.2853, 0.3598, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.9003, 0.6415, 0.8855, 1.0000],\n",
       "         [0.2721, 0.3279, 0.4992, 1.0000],\n",
       "         [0.6728, 0.1692, 0.1441, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.2545, 0.4630, 0.0285, 0.9477],\n",
       "         [0.7755, 0.0924, 0.3518, 0.0279],\n",
       "         [0.5987, 0.2119, 0.8433, 0.2938],\n",
       "         [0.1822, 0.5337, 0.8742, 0.3727]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"F\\xb5\\x90C\\xbfsM\\xd4\\xdf\\xc3\\xf8\\xef\\xc4V\\xe8\\x90\\xd06\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x00\", b'8\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00']\n",
      "Bad pipe message: %s [b'\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'`\\xafG\\x80\\xdf\\x8a\\xe4\\xd5h\\xb3\\xb5\\xdc\\x07k+\\xba*\\xfa\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'J\\x1e\\xb43)44\\r&\\xdb\\x9d\\xa5\\x8e\\xbb{\\xc2\\xfc(\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00']\n",
      "Bad pipe message: %s [b'F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t']\n",
      "Bad pipe message: %s [b'\\xad3\\x00\\xbf\\xef\\x87\\x14\\xfb\\xcd\\x9b\\xb4\\xfc\\x10\\xa2\\xc0\\x11[\\xc3\\x00\\x00\\xa2\\xc0\\x14', b'\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n']\n",
      "Bad pipe message: %s [b'7\\xf2\\x1d\\x9c\\xaa\\xad5\\xbb\\x7fw\\x9b`S([$\\xf2\\xec\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x15\\x03\\x00\\x00\\x02\\x02(']\n",
      "Bad pipe message: %s [b'B\\xaa\\x1dn@\\xc95\\xa8\\x04fh\\xe3\\x9f\\xd3\\x93d\\x91~\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0', b'\\x05']\n",
      "Bad pipe message: %s [b\"\\x126\\xa0\\x07\\x15\\xa2\\x95K\\x80\\xad\\x03\\xe9\\xb0\\x02n\\xd3\\xb84\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\"]\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)\n",
    "b = torch.rand(3,3)\n",
    "c = torch.rand(4,4)\n",
    "values = [a,b,c]\n",
    "\n",
    "size = max(v.size(0) for v in values)\n",
    "value_lst = [v.reshape(-1) for v in values]\n",
    "size_lst = [v.size(0) for v in values]\n",
    "\n",
    "d = torch.arange(0,size).expand(len(values),size,size)\n",
    "d = torch.max(d, d.transpose(-1,-2))\n",
    "d_mask = d < torch.Tensor(size_lst)[:,None,None]\n",
    "torch.ones(len(values),size,size).masked_scatter(d_mask,torch.cat(value_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ff355221f0f6e88a528ead7a22810baa363160013b7ff62dd7bc1a14cdfa164"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
