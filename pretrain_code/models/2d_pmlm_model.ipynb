{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import prediction\n",
    "import sklearn.metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from fairseq import utils, checkpoint_utils\n",
    "from fairseq.checkpoint_utils import load_pretrained_component_from_model\n",
    "from fairseq.models.fconv import FConvDecoder\n",
    "from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\n",
    "from fairseq.models import (\n",
    "    FairseqEncoderModel,\n",
    "    FairseqEncoderDecoderModel,\n",
    "    BaseFairseqModel,\n",
    "    FairseqEncoder,\n",
    "    register_model,\n",
    "    register_model_architecture,\n",
    ")\n",
    "\n",
    "from fairseq.models.masked_lm import MaskedLMEncoder\n",
    "\n",
    "from fairseq.modules import (\n",
    "    LayerNorm,\n",
    "    SinusoidalPositionalEmbedding,\n",
    "    FairseqDropout,\n",
    "    LayerDropModuleList,\n",
    "    PositionalEmbedding,\n",
    ")\n",
    "from fairseq.modules.transformer_sentence_encoder import init_bert_params\n",
    "\n",
    "from typing import Optional, Tuple, Callable, Dict\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from fairseq import utils\n",
    "from fairseq.modules import LayerNorm\n",
    "from fairseq.modules.fairseq_dropout import FairseqDropout\n",
    "from fairseq.modules.quant_noise import quant_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    act_dropout=0.0\n",
    "    activation_fn='gelu'\n",
    "    adam_betas='(0.9,0.98)'\n",
    "    adam_eps=1e-06\n",
    "    add_prev_output_tokens=False\n",
    "    all_gather_list_size=16384\n",
    "    apply_bert_init=True\n",
    "    arch='rna_mlm_base'\n",
    "    attention_dropout=0.1\n",
    "    batch_size=None\n",
    "    batch_size_valid=None\n",
    "    best_checkpoint_metric='loss'\n",
    "    bf16=False\n",
    "    bpe=None\n",
    "    broadcast_buffers=False\n",
    "    bucket_cap_mb=25\n",
    "    checkpoint_shard_count=1\n",
    "    checkpoint_suffix=''\n",
    "    clip_norm=1.0\n",
    "    cpu=False\n",
    "    criterion='rna_mlm'\n",
    "    curriculum=0\n",
    "    data='/home/v-weijieyin/Mydocker/basic-dl-fairseq/rna-pretraining/example/local/../../../rnacentral_data-100_ftvocab_bin/'\n",
    "    data_buffer_size=10\n",
    "    dataset_impl=None\n",
    "    ddp_backend='no_c10d'\n",
    "    device_id=0\n",
    "    disable_validation=False\n",
    "    distributed_backend='nccl'\n",
    "    distributed_init_method=None\n",
    "    distributed_no_spawn=False\n",
    "    distributed_num_procs=1\n",
    "    distributed_port=-1\n",
    "    distributed_rank=0\n",
    "    distributed_world_size=1\n",
    "    distributed_wrapper='DDP'\n",
    "    dropout=0.1\n",
    "    empty_cache_freq=0\n",
    "    encoder_attention_heads=12\n",
    "    encoder_embed_dim=768\n",
    "    encoder_ffn_embed_dim=3072\n",
    "    encoder_layers=12\n",
    "    encoder_learned_pos=False\n",
    "    encoder_normalize_before=True\n",
    "    end_learning_rate=0.0\n",
    "    fast_stat_sync=False\n",
    "    find_unused_parameters=False\n",
    "    finetune_from_model=None\n",
    "    fix_batches_to_gpus=False\n",
    "    fixed_validation_seed=None\n",
    "    force_anneal=None\n",
    "    fp16=True\n",
    "    fp16_init_scale=128\n",
    "    fp16_no_flatten_grads=False\n",
    "    fp16_scale_tolerance=0.0\n",
    "    fp16_scale_window=None\n",
    "    freq_weighted_replacement=False\n",
    "    gen_subset='test'\n",
    "    init_token=None\n",
    "    keep_best_checkpoints=-1\n",
    "    keep_interval_updates=-1\n",
    "    keep_last_epochs=-1\n",
    "    label_padding=-1\n",
    "    leave_unmasked_prob=0.1\n",
    "    localsgd_frequency=3\n",
    "    log_format='simple'\n",
    "    log_interval=10\n",
    "    lr=[0.0001]\n",
    "    lr_scheduler='polynomial_decay'\n",
    "    mask_prob=0.15\n",
    "    mask_whole_words=False\n",
    "    max_epoch=100\n",
    "    max_positions=1024\n",
    "    max_tokens=18432\n",
    "    max_tokens_valid=18432\n",
    "    max_update=0\n",
    "    maximize_best_checkpoint_metric=False\n",
    "    memory_efficient_bf16=False\n",
    "    memory_efficient_fp16=False\n",
    "    min_loss_scale=0.0001\n",
    "    min_lr=-1.0\n",
    "    model_parallel_size=1\n",
    "    no_epoch_checkpoints=False\n",
    "    no_last_checkpoints=False\n",
    "    no_progress_bar=False\n",
    "    no_save=False\n",
    "    no_save_optimizer_state=False\n",
    "    no_seed_provided=False\n",
    "    no_shuffle=False\n",
    "    no_token_positional_embeddings=False\n",
    "    nprocs_per_node=1\n",
    "    num_segment=2\n",
    "    num_shards=1\n",
    "    num_workers=0\n",
    "    optimizer='adam'\n",
    "    optimizer_overrides='{}'\n",
    "    patience=-1\n",
    "    pipeline_balance=None\n",
    "    pipeline_checkpoint='never'\n",
    "    pipeline_chunks=0\n",
    "    pipeline_decoder_balance=None\n",
    "    pipeline_decoder_devices=None\n",
    "    pipeline_devices=None\n",
    "    pipeline_encoder_balance=None\n",
    "    pipeline_encoder_devices=None\n",
    "    pipeline_model_parallel=False\n",
    "    pooler_activation_fn='tanh'\n",
    "    pooler_dropout=0.0\n",
    "    power=1.0\n",
    "    pretrained_file=None\n",
    "    profile=False\n",
    "    quantization_config_path=None\n",
    "    random_token_prob=0.1\n",
    "    required_batch_size_multiple=8\n",
    "    required_seq_len_multiple=1\n",
    "    reset_dataloader=False\n",
    "    reset_lr_scheduler=False\n",
    "    reset_meters=False\n",
    "    reset_optimizer=False\n",
    "    restore_file='checkpoint_last.pt'\n",
    "    sample_break_mode='eos'\n",
    "    save_dir='/home/v-weijieyin/Mydocker/basic-dl-fairseq/rna-pretraining/example/local/../../../rna_mlm-rna_mlm_base-test1/checkpoint'\n",
    "    save_interval=1\n",
    "    save_interval_updates=0\n",
    "    scoring='bleu'\n",
    "    seed=1\n",
    "    sent_loss=True\n",
    "    sentence_avg=False\n",
    "    sentence_class_num=2\n",
    "    separator_token=None\n",
    "    shard_id=0\n",
    "    share_encoder_input_output_embed=True\n",
    "    shorten_data_split_list=''\n",
    "    shorten_method='none'\n",
    "    skip_invalid_size_inputs_valid_test=True\n",
    "    slowmo_algorithm='LocalSGD'\n",
    "    slowmo_momentum=None\n",
    "    stop_time_hours=0\n",
    "    task='rna_mlm'\n",
    "    tensorboard_logdir='/home/v-weijieyin/Mydocker/basic-dl-fairseq/rna-pretraining/example/local/../../../rna_mlm-rna_mlm_base-test1/logs'\n",
    "    threshold_loss_scale=None\n",
    "    tokenizer=None\n",
    "    tokens_per_sample=1024\n",
    "    total_num_update=2000000\n",
    "    tpu=False\n",
    "    train_subset='train'\n",
    "    update_freq=[16]\n",
    "    use_bmuf=False\n",
    "    use_old_adam=False\n",
    "    user_dir='/home/v-weijieyin/Mydocker/basic-dl-fairseq/rna-pretraining/example/local/../../src/rna'\n",
    "    valid_subset='valid'\n",
    "    validate_after_updates=0\n",
    "    validate_interval=1\n",
    "    validate_interval_updates=0\n",
    "    warmup_updates=20000\n",
    "    weight_decay=0.01\n",
    "    zero_sharding='none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def base_architecture(args):\n",
    "    args.dropout = getattr(args, 'dropout', 0.1)\n",
    "    args.attention_dropout = getattr(args, 'attention_dropout', 0.1)\n",
    "    args.act_dropout = getattr(args, 'act_dropout', 0.0)\n",
    "\n",
    "    args.share_encoder_input_output_embed = getattr(\n",
    "        args, 'share_encoder_input_output_embed', True)\n",
    "    args.no_token_positional_embeddings = getattr(\n",
    "        args, 'no_token_positional_embeddings', False)\n",
    "    args.encoder_learned_pos = getattr(args, 'encoder_learned_pos', False)\n",
    "    args.num_segment = getattr(args, 'num_segment', 2)\n",
    "\n",
    "    args.sentence_class_num = getattr(args, 'sentence_class_num', 2)\n",
    "    args.sent_loss = getattr(args, 'sent_loss', True)\n",
    "\n",
    "    args.apply_bert_init = getattr(args, 'apply_bert_init', True)\n",
    "\n",
    "    args.activation_fn = getattr(args, 'activation_fn', 'gelu')\n",
    "    args.pooler_activation_fn = getattr(args, 'pooler_activation_fn', 'tanh')\n",
    "    args.encoder_normalize_before = getattr(args, 'encoder_normalize_before', True)\n",
    "\n",
    "    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 768)\n",
    "    args.encoder_layers = getattr(args, 'encoder_layers', 12)\n",
    "    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 12)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention.\n",
    "    See \"Attention Is All You Need\" for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        dropout=0.0,\n",
    "        bias=True,\n",
    "        add_bias_kv=False,\n",
    "        add_zero_attn=False,\n",
    "        self_attention=False,\n",
    "        encoder_decoder_attention=False,\n",
    "        q_noise=0.0,\n",
    "        qn_block_size=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_module = FairseqDropout(\n",
    "            dropout, module_name=self.__class__.__name__\n",
    "        )\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "        self.self_attention = self_attention\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "\n",
    "        assert not self.self_attention or self.qkv_same_dim, (\n",
    "            \"Self-attention requires query, key and \" \"value to be of the same size\"\n",
    "        )\n",
    "\n",
    "        self.k_proj = quant_noise(\n",
    "            nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
    "        )\n",
    "        self.v_proj = quant_noise(\n",
    "            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n",
    "        )\n",
    "        self.q_proj = quant_noise(\n",
    "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
    "        )\n",
    "\n",
    "        self.out_proj = quant_noise(\n",
    "            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n",
    "        )\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.onnx_trace = False\n",
    "        self.tpu = False\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    def prepare_for_tpu_(self, **kwargs):\n",
    "        self.tpu = True\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.qkv_same_dim:\n",
    "            # Empirically observed the convergence to be much better with\n",
    "            # the scaled initialization\n",
    "            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n",
    "            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n",
    "            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "            nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "            nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0.0)\n",
    "        if self.bias_k is not None:\n",
    "            nn.init.xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            nn.init.xavier_normal_(self.bias_v)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key: Optional[Tensor],\n",
    "        value: Optional[Tensor],\n",
    "        twod_tokens: Optional[Tensor],\n",
    "        is_twod: Optional[bool],\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        need_weights: bool = True,\n",
    "        static_kv: bool = False,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        before_softmax: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "        Args:\n",
    "            key_padding_mask (ByteTensor, optional): mask to exclude\n",
    "                keys that are pads, of shape `(batch, src_len)`, where\n",
    "                padding elements are indicated by 1s.\n",
    "            need_weights (bool, optional): return the attention weights,\n",
    "                averaged over heads (default: False).\n",
    "            attn_mask (ByteTensor, optional): typically used to\n",
    "                implement causal attention, where the mask prevents the\n",
    "                attention from looking forward in time (default: None).\n",
    "            before_softmax (bool, optional): return the raw attention\n",
    "                weights and values before the attention softmax.\n",
    "            need_head_weights (bool, optional): return the attention\n",
    "                weights for each head. Implies *need_weights*. Default:\n",
    "                return the average attention weights over all heads.\n",
    "        \"\"\"\n",
    "        if need_head_weights:\n",
    "            need_weights = True\n",
    "\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
    "\n",
    "        if (\n",
    "            not self.onnx_trace\n",
    "            and not self.tpu  # don't use PyTorch version on TPUs\n",
    "            and incremental_state is None\n",
    "            and not static_kv\n",
    "            # A workaround for quantization to work. Otherwise JIT compilation\n",
    "            # treats bias in linear module as method.\n",
    "            and not torch.jit.is_scripting()\n",
    "        ):\n",
    "            assert key is not None and value is not None\n",
    "            # print(\"here\")\n",
    "            return F.multi_head_attention_forward(\n",
    "                query,\n",
    "                key,\n",
    "                value,\n",
    "                self.embed_dim,\n",
    "                self.num_heads,\n",
    "                torch.empty([0]),\n",
    "                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n",
    "                self.bias_k,\n",
    "                self.bias_v,\n",
    "                self.add_zero_attn,\n",
    "                is_twod,\n",
    "                self.dropout_module.p,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "                self.training or self.dropout_module.apply_during_inference,\n",
    "                key_padding_mask,\n",
    "                need_weights,\n",
    "                attn_mask,\n",
    "                use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj.weight,\n",
    "                k_proj_weight=self.k_proj.weight,\n",
    "                v_proj_weight=self.v_proj.weight,\n",
    "                twod_tokens = twod_tokens,\n",
    "            )\n",
    "            \n",
    "            \n",
    "\n",
    "        # print(\"not here\")\n",
    "        if incremental_state is not None:\n",
    "            saved_state = self._get_input_buffer(incremental_state)\n",
    "            if saved_state is not None and \"prev_key\" in saved_state:\n",
    "                # previous time steps are cached - no need to recompute\n",
    "                # key and value if they are static\n",
    "                if static_kv:\n",
    "                    assert self.encoder_decoder_attention and not self.self_attention\n",
    "                    key = value = None\n",
    "        else:\n",
    "            saved_state = None\n",
    "\n",
    "        if self.self_attention:\n",
    "            q = self.q_proj(query)\n",
    "            k = self.k_proj(query)\n",
    "            v = self.v_proj(query)\n",
    "        elif self.encoder_decoder_attention:\n",
    "            # encoder-decoder attention\n",
    "            q = self.q_proj(query)\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = v = None\n",
    "            else:\n",
    "                k = self.k_proj(key)\n",
    "                v = self.v_proj(key)\n",
    "\n",
    "        else:\n",
    "            assert key is not None and value is not None\n",
    "            q = self.q_proj(query)\n",
    "            k = self.k_proj(key)\n",
    "            v = self.v_proj(value)\n",
    "        q *= self.scaling\n",
    "\n",
    "        if self.bias_k is not None:\n",
    "            assert self.bias_v is not None\n",
    "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat(\n",
    "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
    "                )\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [\n",
    "                        key_padding_mask,\n",
    "                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "        q = (\n",
    "            q.contiguous()\n",
    "            .view(tgt_len, bsz * self.num_heads, self.head_dim)\n",
    "            .transpose(0, 1)\n",
    "        )\n",
    "        if k is not None:\n",
    "            k = (\n",
    "                k.contiguous()\n",
    "                .view(-1, bsz * self.num_heads, self.head_dim)\n",
    "                .transpose(0, 1)\n",
    "            )\n",
    "        if v is not None:\n",
    "            v = (\n",
    "                v.contiguous()\n",
    "                .view(-1, bsz * self.num_heads, self.head_dim)\n",
    "                .transpose(0, 1)\n",
    "            )\n",
    "\n",
    "        if saved_state is not None:\n",
    "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
    "            if \"prev_key\" in saved_state:\n",
    "                _prev_key = saved_state[\"prev_key\"]\n",
    "                assert _prev_key is not None\n",
    "                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n",
    "                if static_kv:\n",
    "                    k = prev_key\n",
    "                else:\n",
    "                    assert k is not None\n",
    "                    k = torch.cat([prev_key, k], dim=1)\n",
    "            if \"prev_value\" in saved_state:\n",
    "                _prev_value = saved_state[\"prev_value\"]\n",
    "                assert _prev_value is not None\n",
    "                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n",
    "                if static_kv:\n",
    "                    v = prev_value\n",
    "                else:\n",
    "                    assert v is not None\n",
    "                    v = torch.cat([prev_value, v], dim=1)\n",
    "            prev_key_padding_mask: Optional[Tensor] = None\n",
    "            if \"prev_key_padding_mask\" in saved_state:\n",
    "                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n",
    "            assert k is not None and v is not None\n",
    "            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                prev_key_padding_mask=prev_key_padding_mask,\n",
    "                batch_size=bsz,\n",
    "                src_len=k.size(1),\n",
    "                static_kv=static_kv,\n",
    "            )\n",
    "\n",
    "            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
    "            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
    "            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n",
    "            # In this branch incremental_state is never None\n",
    "            assert incremental_state is not None\n",
    "            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n",
    "        assert k is not None\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        # This is part of a workaround to get around fork/join parallelism\n",
    "        # not supporting Optional types.\n",
    "        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n",
    "            key_padding_mask = None\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            assert v is not None\n",
    "            src_len += 1\n",
    "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
    "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat(\n",
    "                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n",
    "                )\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [\n",
    "                        key_padding_mask,\n",
    "                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n",
    "                            key_padding_mask\n",
    "                        ),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n",
    "\n",
    "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if self.onnx_trace:\n",
    "                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n",
    "            attn_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # don't attend to padding symbols\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            if not self.tpu:\n",
    "                attn_weights = attn_weights.masked_fill(\n",
    "                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n",
    "                    float(\"-inf\"),\n",
    "                )\n",
    "            else:\n",
    "                attn_weights = attn_weights.transpose(0, 2)\n",
    "                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n",
    "                attn_weights = attn_weights.transpose(0, 2)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if before_softmax:\n",
    "            return attn_weights, v\n",
    "\n",
    "        attn_weights_float = utils.softmax(\n",
    "            attn_weights, dim=-1, onnx_trace=self.onnx_trace\n",
    "        )\n",
    "        attn_weights = attn_weights_float.type_as(attn_weights)\n",
    "        attn_probs = self.dropout_module(attn_weights)\n",
    "\n",
    "        assert v is not None\n",
    "        attn = torch.bmm(attn_probs, v)\n",
    "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
    "        if self.onnx_trace and attn.size(1) == 1:\n",
    "            # when ONNX tracing a single decoder step (sequence length == 1)\n",
    "            # the transpose is a no-op copy before view, thus unnecessary\n",
    "            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        else:\n",
    "            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "        attn_weights: Optional[Tensor] = None\n",
    "        if need_weights:\n",
    "            attn_weights = attn_weights_float.view(\n",
    "                bsz, self.num_heads, tgt_len, src_len\n",
    "            ).transpose(1, 0)\n",
    "            if not need_head_weights:\n",
    "                # average attention weights over heads\n",
    "                attn_weights = attn_weights.mean(dim=0)\n",
    "\n",
    "        return attn, attn_weights\n",
    "\n",
    "    @staticmethod\n",
    "    def _append_prev_key_padding_mask(\n",
    "        key_padding_mask: Optional[Tensor],\n",
    "        prev_key_padding_mask: Optional[Tensor],\n",
    "        batch_size: int,\n",
    "        src_len: int,\n",
    "        static_kv: bool,\n",
    "    ) -> Optional[Tensor]:\n",
    "        # saved key padding masks have shape (bsz, seq_len)\n",
    "        if prev_key_padding_mask is not None and static_kv:\n",
    "            new_key_padding_mask = prev_key_padding_mask\n",
    "        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n",
    "            new_key_padding_mask = torch.cat(\n",
    "                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n",
    "            )\n",
    "        # During incremental decoding, as the padding token enters and\n",
    "        # leaves the frame, there will be a time when prev or current\n",
    "        # is None\n",
    "        elif prev_key_padding_mask is not None:\n",
    "            filler = torch.zeros(\n",
    "                (batch_size, src_len - prev_key_padding_mask.size(1)),\n",
    "                device=prev_key_padding_mask.device,\n",
    "            )\n",
    "            new_key_padding_mask = torch.cat(\n",
    "                [prev_key_padding_mask.float(), filler.float()], dim=1\n",
    "            )\n",
    "        elif key_padding_mask is not None:\n",
    "            filler = torch.zeros(\n",
    "                (batch_size, src_len - key_padding_mask.size(1)),\n",
    "                device=key_padding_mask.device,\n",
    "            )\n",
    "            new_key_padding_mask = torch.cat(\n",
    "                [filler.float(), key_padding_mask.float()], dim=1\n",
    "            )\n",
    "        else:\n",
    "            new_key_padding_mask = prev_key_padding_mask\n",
    "        return new_key_padding_mask\n",
    "\n",
    "    @torch.jit.export\n",
    "    def reorder_incremental_state(\n",
    "        self,\n",
    "        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
    "        new_order: Tensor,\n",
    "    ):\n",
    "        \"\"\"Reorder buffered internal state (for incremental generation).\"\"\"\n",
    "        input_buffer = self._get_input_buffer(incremental_state)\n",
    "        if input_buffer is not None:\n",
    "            for k in input_buffer.keys():\n",
    "                input_buffer_k = input_buffer[k]\n",
    "                if input_buffer_k is not None:\n",
    "                    if self.encoder_decoder_attention and input_buffer_k.size(\n",
    "                        0\n",
    "                    ) == new_order.size(0):\n",
    "                        break\n",
    "                    input_buffer[k] = input_buffer_k.index_select(0, new_order)\n",
    "            incremental_state = self._set_input_buffer(incremental_state, input_buffer)\n",
    "        return incremental_state\n",
    "\n",
    "    def _get_input_buffer(\n",
    "        self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n",
    "    ) -> Dict[str, Optional[Tensor]]:\n",
    "        result = self.get_incremental_state(incremental_state, \"attn_state\")\n",
    "        if result is not None:\n",
    "            return result\n",
    "        else:\n",
    "            empty_result: Dict[str, Optional[Tensor]] = {}\n",
    "            return empty_result\n",
    "\n",
    "    def _set_input_buffer(\n",
    "        self,\n",
    "        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n",
    "        buffer: Dict[str, Optional[Tensor]],\n",
    "    ):\n",
    "        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n",
    "\n",
    "    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n",
    "        return attn_weights\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        prefix = name + \".\" if name != \"\" else \"\"\n",
    "        items_to_add = {}\n",
    "        keys_to_remove = []\n",
    "        for k in state_dict.keys():\n",
    "            if k.endswith(prefix + \"in_proj_weight\"):\n",
    "                # in_proj_weight used to be q + k + v with same dimensions\n",
    "                dim = int(state_dict[k].shape[0] / 3)\n",
    "                items_to_add[prefix + \"q_proj.weight\"] = state_dict[k][:dim]\n",
    "                items_to_add[prefix + \"k_proj.weight\"] = state_dict[k][dim : 2 * dim]\n",
    "                items_to_add[prefix + \"v_proj.weight\"] = state_dict[k][2 * dim :]\n",
    "\n",
    "                keys_to_remove.append(k)\n",
    "\n",
    "                k_bias = prefix + \"in_proj_bias\"\n",
    "                if k_bias in state_dict.keys():\n",
    "                    dim = int(state_dict[k].shape[0] / 3)\n",
    "                    items_to_add[prefix + \"q_proj.bias\"] = state_dict[k_bias][:dim]\n",
    "                    items_to_add[prefix + \"k_proj.bias\"] = state_dict[k_bias][\n",
    "                        dim : 2 * dim\n",
    "                    ]\n",
    "                    items_to_add[prefix + \"v_proj.bias\"] = state_dict[k_bias][2 * dim :]\n",
    "\n",
    "                    keys_to_remove.append(prefix + \"in_proj_bias\")\n",
    "\n",
    "        for k in keys_to_remove:\n",
    "            del state_dict[k]\n",
    "\n",
    "        for key, value in items_to_add.items():\n",
    "            state_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSentenceEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 768,\n",
    "        ffn_embedding_dim: int = 3072,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        activation_dropout: float = 0.1,\n",
    "        activation_fn: str = \"relu\",\n",
    "        export: bool = False,\n",
    "        q_noise: float = 0.0,\n",
    "        qn_block_size: int = 8,\n",
    "        init_fn: Callable = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if init_fn is not None:\n",
    "            init_fn()\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_module = FairseqDropout(\n",
    "            dropout, module_name=self.__class__.__name__\n",
    "        )\n",
    "        self.activation_dropout_module = FairseqDropout(\n",
    "            activation_dropout, module_name=self.__class__.__name__\n",
    "        )\n",
    "\n",
    "        # Initialize blocks\n",
    "        self.activation_fn = utils.get_activation_fn(activation_fn)\n",
    "        self.self_attn = self.build_self_attention(\n",
    "            self.embedding_dim,\n",
    "            num_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            self_attention=True,\n",
    "            q_noise=q_noise,\n",
    "            qn_block_size=qn_block_size,\n",
    "        )\n",
    "\n",
    "        # layer norm associated with the self attention layer\n",
    "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
    "\n",
    "        self.fc1 = self.build_fc1(\n",
    "            self.embedding_dim,\n",
    "            ffn_embedding_dim,\n",
    "            q_noise=q_noise,\n",
    "            qn_block_size=qn_block_size,\n",
    "        )\n",
    "        self.fc2 = self.build_fc2(\n",
    "            ffn_embedding_dim,\n",
    "            self.embedding_dim,\n",
    "            q_noise=q_noise,\n",
    "            qn_block_size=qn_block_size,\n",
    "        )\n",
    "\n",
    "        # layer norm associated with the position wise feed-forward NN\n",
    "        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
    "\n",
    "    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n",
    "        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n",
    "\n",
    "    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n",
    "        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n",
    "\n",
    "    def build_self_attention(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_attention_heads,\n",
    "        dropout,\n",
    "        self_attention,\n",
    "        q_noise,\n",
    "        qn_block_size,\n",
    "    ):\n",
    "        return MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_attention_heads,\n",
    "            dropout=dropout,\n",
    "            self_attention=True,\n",
    "            q_noise=q_noise,\n",
    "            qn_block_size=qn_block_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        twod_tokens: torch.Tensor,\n",
    "        is_twod: bool, \n",
    "        self_attn_mask: Optional[torch.Tensor] = None,\n",
    "        self_attn_padding_mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LayerNorm is applied either before or after the self-attention/ffn\n",
    "        modules similar to the original Transformer implementation.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x, attn, twod_tokens_new = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            twod_tokens = twod_tokens, \n",
    "            is_twod = is_twod,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            need_weights=False,\n",
    "            attn_mask=self_attn_mask,\n",
    "        )\n",
    "        x = self.dropout_module(x)\n",
    "        x = residual + x\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.activation_dropout_module(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout_module(x)\n",
    "        x = residual + x\n",
    "        x = self.final_layer_norm(x)\n",
    "        return x, attn, twod_tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_bert_params(module):\n",
    "    \"\"\"\n",
    "    Initialize the weights specific to the BERT Model.\n",
    "    This overrides the default initializations depending on the specified arguments.\n",
    "        1. If normal_init_linear_weights is set then weights of linear\n",
    "           layer will be initialized using the normal distribution and\n",
    "           bais will be set to the specified value.\n",
    "        2. If normal_init_embed_weights is set then weights of embedding\n",
    "           layer will be initialized using the normal distribution.\n",
    "        3. If normal_init_proj_weights is set then weights of\n",
    "           in_project_weight for MultiHeadAttention initialized using\n",
    "           the normal distribution (to be validated).\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(module, nn.Linear):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if module.padding_idx is not None:\n",
    "            module.weight.data[module.padding_idx].zero_()\n",
    "    if isinstance(module, MultiheadAttention):\n",
    "        module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "class TransformerSentenceEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation for a Bi-directional Transformer based Sentence Encoder used\n",
    "    in BERT/XLM style pre-trained models.\n",
    "    This first computes the token embedding using the token embedding matrix,\n",
    "    position embeddings (if specified) and segment embeddings\n",
    "    (if specified). After applying the specified number of\n",
    "    TransformerEncoderLayers, it outputs all the internal states of the\n",
    "    encoder as well as the final representation associated with the first\n",
    "    token (usually CLS token).\n",
    "    Input:\n",
    "        - tokens: B x T matrix representing sentences\n",
    "        - segment_labels: B x T matrix representing segment label for tokens\n",
    "    Output:\n",
    "        - a tuple of the following:\n",
    "            - a list of internal model states used to compute the\n",
    "              predictions where each tensor has shape T x B x C\n",
    "            - sentence representation associated with first input token\n",
    "              in format B x C.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        padding_idx: int,\n",
    "        vocab_size: int,\n",
    "        num_encoder_layers: int = 6,\n",
    "        embedding_dim: int = 768,\n",
    "        ffn_embedding_dim: int = 3072,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        activation_dropout: float = 0.1,\n",
    "        layerdrop: float = 0.0,\n",
    "        max_seq_len: int = 256,\n",
    "        num_segments: int = 2,\n",
    "        use_position_embeddings: bool = True,\n",
    "        offset_positions_by_padding: bool = True,\n",
    "        encoder_normalize_before: bool = False,\n",
    "        apply_bert_init: bool = False,\n",
    "        activation_fn: str = \"relu\",\n",
    "        learned_pos_embedding: bool = True,\n",
    "        embed_scale: float = None,\n",
    "        freeze_embeddings: bool = False,\n",
    "        n_trans_layers_to_freeze: int = 0,\n",
    "        export: bool = False,\n",
    "        traceable: bool = False,\n",
    "        q_noise: float = 0.0,\n",
    "        qn_block_size: int = 8,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_module = FairseqDropout(\n",
    "            dropout, module_name=self.__class__.__name__\n",
    "        )\n",
    "        self.layerdrop = layerdrop\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_segments = num_segments\n",
    "        self.use_position_embeddings = use_position_embeddings\n",
    "        self.apply_bert_init = apply_bert_init\n",
    "        self.learned_pos_embedding = learned_pos_embedding\n",
    "        self.traceable = traceable\n",
    "        self.tpu = False  # whether we're on TPU\n",
    "\n",
    "        self.embed_tokens = self.build_embedding(\n",
    "            self.vocab_size, self.embedding_dim, self.padding_idx\n",
    "        )\n",
    "        self.embed_scale = embed_scale\n",
    "\n",
    "        if q_noise > 0:\n",
    "            self.quant_noise = apply_quant_noise_(\n",
    "                nn.Linear(self.embedding_dim, self.embedding_dim, bias=False),\n",
    "                q_noise,\n",
    "                qn_block_size,\n",
    "            )\n",
    "        else:\n",
    "            self.quant_noise = None\n",
    "\n",
    "        self.segment_embeddings = (\n",
    "            nn.Embedding(self.num_segments, self.embedding_dim, padding_idx=None)\n",
    "            if self.num_segments > 0\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.embed_positions = (\n",
    "            PositionalEmbedding(\n",
    "                self.max_seq_len,\n",
    "                self.embedding_dim,\n",
    "                padding_idx=(self.padding_idx if offset_positions_by_padding else None),\n",
    "                learned=self.learned_pos_embedding,\n",
    "            )\n",
    "            if self.use_position_embeddings\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        if self.layerdrop > 0.0:\n",
    "            self.layers = LayerDropModuleList(p=self.layerdrop)\n",
    "        else:\n",
    "            self.layers = nn.ModuleList([])\n",
    "        self.layers.extend(\n",
    "            [\n",
    "                self.build_transformer_sentence_encoder_layer(\n",
    "                    embedding_dim=self.embedding_dim,\n",
    "                    ffn_embedding_dim=ffn_embedding_dim,\n",
    "                    num_attention_heads=num_attention_heads,\n",
    "                    dropout=self.dropout_module.p,\n",
    "                    attention_dropout=attention_dropout,\n",
    "                    activation_dropout=activation_dropout,\n",
    "                    activation_fn=activation_fn,\n",
    "                    export=export,\n",
    "                    q_noise=q_noise,\n",
    "                    qn_block_size=qn_block_size,\n",
    "                )\n",
    "                for _ in range(num_encoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if encoder_normalize_before:\n",
    "            self.emb_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
    "        else:\n",
    "            self.emb_layer_norm = None\n",
    "\n",
    "        # Apply initialization of model params after building the model\n",
    "        if self.apply_bert_init:\n",
    "            self.apply(init_bert_params)\n",
    "\n",
    "        def freeze_module_params(m):\n",
    "            if m is not None:\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        if freeze_embeddings:\n",
    "            freeze_module_params(self.embed_tokens)\n",
    "            freeze_module_params(self.segment_embeddings)\n",
    "            freeze_module_params(self.embed_positions)\n",
    "            freeze_module_params(self.emb_layer_norm)\n",
    "\n",
    "        for layer in range(n_trans_layers_to_freeze):\n",
    "            freeze_module_params(self.layers[layer])\n",
    "\n",
    "    def build_embedding(self, vocab_size, embedding_dim, padding_idx):\n",
    "        return nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
    "\n",
    "    def build_transformer_sentence_encoder_layer(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        ffn_embedding_dim,\n",
    "        num_attention_heads,\n",
    "        dropout,\n",
    "        attention_dropout,\n",
    "        activation_dropout,\n",
    "        activation_fn,\n",
    "        export,\n",
    "        q_noise,\n",
    "        qn_block_size,\n",
    "    ):\n",
    "        return TransformerSentenceEncoderLayer(\n",
    "            embedding_dim=embedding_dim,\n",
    "            ffn_embedding_dim=ffn_embedding_dim,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            activation_dropout=activation_dropout,\n",
    "            activation_fn=activation_fn,\n",
    "            export=export,\n",
    "            q_noise=q_noise,\n",
    "            qn_block_size=qn_block_size,\n",
    "        )\n",
    "\n",
    "    def prepare_for_tpu_(self, **kwargs):\n",
    "        self.tpu = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "        # twod_tokens: torch.Tensor,\n",
    "        segment_labels: torch.Tensor = None,\n",
    "        last_state_only: bool = False,\n",
    "        is_twod: bool = False,\n",
    "        twod_tokens: Optional[torch.Tensor] = None,\n",
    "        positions: Optional[torch.Tensor] = None,\n",
    "        token_embeddings: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        # compute padding mask. This is needed for multi-head attention\n",
    "        padding_mask = tokens.eq(self.padding_idx)\n",
    "        if not self.traceable and not self.tpu and not padding_mask.any():\n",
    "            padding_mask = None\n",
    "\n",
    "        if token_embeddings is not None:\n",
    "            x = token_embeddings\n",
    "        else:\n",
    "            x = self.embed_tokens(tokens)\n",
    "\n",
    "        if self.embed_scale is not None:\n",
    "            x = x * self.embed_scale\n",
    "\n",
    "        if self.embed_positions is not None:\n",
    "            x = x + self.embed_positions(tokens, positions=positions)\n",
    "\n",
    "        if self.segment_embeddings is not None and segment_labels is not None:\n",
    "            x = x + self.segment_embeddings(segment_labels)\n",
    "\n",
    "        if self.quant_noise is not None:\n",
    "            x = self.quant_noise(x)\n",
    "\n",
    "        if self.emb_layer_norm is not None:\n",
    "            x = self.emb_layer_norm(x)\n",
    "\n",
    "        x = self.dropout_module(x)\n",
    "\n",
    "        # account for padding while computing the representation\n",
    "        if padding_mask is not None:\n",
    "            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        inner_states = []\n",
    "        if not last_state_only:\n",
    "            inner_states.append(x)\n",
    "        # print(\"twod_tokens:\",twod_tokens)\n",
    "        for layer in self.layers:\n",
    "            x, _, twod_tokens = layer(x, self_attn_padding_mask=padding_mask, twod_tokens= twod_tokens, is_twod = is_twod)\n",
    "            if not last_state_only:\n",
    "                inner_states.append(x)\n",
    "\n",
    "        sentence_rep = x[0, :, :]\n",
    "\n",
    "        if last_state_only:\n",
    "            inner_states = [x]\n",
    "\n",
    "        if self.traceable:\n",
    "            return torch.stack(inner_states), sentence_rep\n",
    "        else:\n",
    "            return inner_states, sentence_rep, twod_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAMaskedLMEncoder(MaskedLMEncoder):\n",
    "\n",
    "    def __init__(self, args, dictionary):\n",
    "        super().__init__(args, dictionary)\n",
    "\n",
    "        self.padding_idx = dictionary.pad()\n",
    "        self.vocab_size = dictionary.__len__()\n",
    "        self.max_positions = args.max_positions\n",
    "\n",
    "        self.sentence_encoder = TransformerSentenceEncoder(\n",
    "            padding_idx=self.padding_idx,\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_encoder_layers=args.encoder_layers,\n",
    "            embedding_dim=args.encoder_embed_dim,\n",
    "            ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
    "            num_attention_heads=args.encoder_attention_heads,\n",
    "            dropout=args.dropout,\n",
    "            attention_dropout=args.attention_dropout,\n",
    "            activation_dropout=args.act_dropout,\n",
    "            max_seq_len=self.max_positions,\n",
    "            num_segments=args.num_segment,\n",
    "            use_position_embeddings=not args.no_token_positional_embeddings,\n",
    "            encoder_normalize_before=args.encoder_normalize_before,\n",
    "            apply_bert_init=args.apply_bert_init,\n",
    "            activation_fn=args.activation_fn,\n",
    "            learned_pos_embedding=args.encoder_learned_pos,\n",
    "        )\n",
    "\n",
    "        self.share_input_output_embed = args.share_encoder_input_output_embed\n",
    "        self.embed_out = None\n",
    "        self.sentence_projection_layer = None\n",
    "        self.sentence_out_dim = args.sentence_class_num\n",
    "        self.lm_output_learned_bias = None\n",
    "\n",
    "        self.load_softmax = not getattr(args, \"remove_head\", False)\n",
    "\n",
    "        self.masked_lm_pooler = nn.Linear(\n",
    "            args.encoder_embed_dim, args.encoder_embed_dim\n",
    "        )\n",
    "        self.pooler_activation = utils.get_activation_fn(args.pooler_activation_fn)\n",
    "\n",
    "        self.lm_head_transform_weight = nn.Linear(\n",
    "            args.encoder_embed_dim, args.encoder_embed_dim\n",
    "        )\n",
    "        self.activation_fn = utils.get_activation_fn(args.activation_fn)\n",
    "        self.layer_norm = LayerNorm(args.encoder_embed_dim)\n",
    "\n",
    "        self.lm_output_learned_bias = None\n",
    "        if self.load_softmax:\n",
    "            self.lm_output_learned_bias = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "\n",
    "            if not self.share_input_output_embed:\n",
    "                self.embed_out = nn.Linear(\n",
    "                    args.encoder_embed_dim, self.vocab_size, bias=False\n",
    "                )\n",
    "\n",
    "            if args.sent_loss:\n",
    "                self.sentence_projection_layer = nn.Linear(\n",
    "                    args.encoder_embed_dim, self.sentence_out_dim, bias=False\n",
    "                )\n",
    "\n",
    "    def remove_head(self):\n",
    "        self.lm_output_learned_bias = None\n",
    "        self.embed_out = None\n",
    "\n",
    "    def forward(self, src_tokens, twod_tokens=None, is_twod=False, segment_labels=None, masked_tokens=None,\n",
    "                extra_only=False, masked_only=False, **unused):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - src_tokens: B x T matrix representing sentences\n",
    "            - segment_labels: B x T matrix representing segment label for tokens\n",
    "        Returns:\n",
    "            - a tuple of the following:\n",
    "                - logits for predictions in format B x T x C to be used in\n",
    "                  softmax afterwards\n",
    "                - a dictionary of additional data, where 'pooled_output' contains\n",
    "                  the representation for classification_token and 'inner_states'\n",
    "                  is a list of internal model states used to compute the\n",
    "                  predictions (similar in ELMO). 'sentence_logits'\n",
    "                  is the prediction logit for NSP task and is only computed if\n",
    "                  this is specified in the input arguments.\n",
    "        \"\"\"\n",
    "\n",
    "        inner_states, sentence_rep, last_attn_bias = self.sentence_encoder(\n",
    "            src_tokens,\n",
    "            twod_tokens=twod_tokens,\n",
    "            is_twod=is_twod,\n",
    "            segment_labels=segment_labels,\n",
    "        ) # inner_states[-1]: (T, B, C)\n",
    "\n",
    "        if extra_only:\n",
    "            x = None\n",
    "        else:\n",
    "            x = inner_states[-1].transpose(0, 1)  # (T, B, C) -> (B, T, C)\n",
    "\n",
    "            sentence_logits = x[0, :, :]\n",
    "\n",
    "            # project masked tokens only\n",
    "            if masked_tokens is not None:  # masked_tokens: (B, T)\n",
    "                x = x[masked_tokens, :]\n",
    "\n",
    "            x = self.layer_norm(self.activation_fn(self.lm_head_transform_weight(x)))\n",
    "\n",
    "            if self.share_input_output_embed and hasattr(\n",
    "                    self.sentence_encoder.embed_tokens, \"weight\"\n",
    "            ):\n",
    "                x = F.linear(x, self.sentence_encoder.embed_tokens.weight)\n",
    "            elif self.embed_out is not None:\n",
    "                x = self.embed_out(x)\n",
    "                \n",
    "            if self.lm_output_learned_bias is not None:\n",
    "                x = x + self.lm_output_learned_bias\n",
    "            \n",
    "        if not masked_only:\n",
    "            pooled_output = self.pooler_activation(self.masked_lm_pooler(sentence_rep))\n",
    "        else:\n",
    "            del inner_states\n",
    "            inner_states = None\n",
    "            pooled_output = None  # for saving GPU storage and calculation time\n",
    "\n",
    "        return x, last_attn_bias, {\n",
    "            \"inner_states\": inner_states,\n",
    "            \"pooled_output\": pooled_output,\n",
    "            \"sentence_logits\": sentence_logits,\n",
    "        }\n",
    "\n",
    "    def max_positions(self):\n",
    "        return self.max_positions\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        if isinstance(\n",
    "                self.sentence_encoder.embed_positions, SinusoidalPositionalEmbedding\n",
    "        ):\n",
    "            state_dict[name + \".sentence_encoder.embed_positions._float_tensor\"] \\\n",
    "                = torch.FloatTensor(1)\n",
    "        if not self.load_softmax:\n",
    "            for k in list(state_dict.keys()):\n",
    "                if (\n",
    "                        \"embed_out.weight\" in k or \n",
    "                        \"sentence_projection_layer.weight\" in k or \n",
    "                        \"lm_output_learned_bias\" in k\n",
    "                ):\n",
    "                    del state_dict[k]\n",
    "        return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNABaseModel(BaseFairseqModel):\n",
    "    \"\"\"Base class for encoder-only models for protein.\n",
    "\n",
    "    Args:\n",
    "        encoder (FairseqEncoder): the encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, encoder, classification_heads=nn.ModuleDict()):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.args = args\n",
    "        self.classification_heads = classification_heads\n",
    "\n",
    "        assert isinstance(self.encoder, FairseqEncoder)\n",
    "        if getattr(args, 'apply_bert_init', False):\n",
    "            self.apply(init_bert_params)  # TODO whether triage bug\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        \"\"\"Add model-specific arguments to the parser.\"\"\"\n",
    "        parser.add_argument('--pooler_dropout', type=float, default=0.,\n",
    "                            help='used in classification head')\n",
    "\n",
    "        # Arguments related to dropout\n",
    "        parser.add_argument('--dropout', type=float, metavar='D',\n",
    "                            help='dropout probability')\n",
    "        parser.add_argument('--attention-dropout', type=float,\n",
    "                            metavar='D', help='dropout probability for'\n",
    "                                              ' attention weights')\n",
    "        parser.add_argument('--act-dropout', type=float,\n",
    "                            metavar='D', help='dropout probability after'\n",
    "                                              ' activation in FFN')\n",
    "\n",
    "        # Arguments related to hidden states and self-attention\n",
    "        parser.add_argument('--encoder-ffn-embed-dim', type=int, metavar='N',\n",
    "                            help='encoder embedding dimension for FFN')\n",
    "        parser.add_argument('--encoder-layers', type=int, metavar='N',\n",
    "                            help='num encoder layers')\n",
    "        parser.add_argument('--encoder-attention-heads', type=int, metavar='N',\n",
    "                            help='num encoder attention heads')\n",
    "\n",
    "        # Arguments related to input and output embeddings\n",
    "        parser.add_argument('--encoder-embed-dim', type=int, metavar='N',\n",
    "                            help='encoder embedding dimension')\n",
    "        parser.add_argument('--share-encoder-input-output-embed',\n",
    "                            action='store_true', help='share encoder input'\n",
    "                                                      ' and output embeddings')\n",
    "        parser.add_argument('--encoder-learned-pos', action='store_true',\n",
    "                            help='use learned positional embeddings in the encoder')\n",
    "        parser.add_argument('--no-token-positional-embeddings',\n",
    "                            action='store_true',\n",
    "                            help='if set, disables positional embeddings'\n",
    "                                 ' (outside self attention)')\n",
    "        parser.add_argument('--num-segment', type=int, metavar='N',\n",
    "                            help='num segment in the input')\n",
    "        parser.add_argument('--max-positions', type=int,\n",
    "                            help='number of positional embeddings to learn')\n",
    "\n",
    "        # Arguments related to sentence level prediction\n",
    "        parser.add_argument('--sentence-class-num', type=int, metavar='N',\n",
    "                            help='number of classes for sentence task')\n",
    "        parser.add_argument('--sent-loss', action='store_true', help='if set,'\n",
    "                                                                     ' calculate sentence level predictions')\n",
    "\n",
    "        # Arguments related to parameter initialization\n",
    "        parser.add_argument('--apply-bert-init', action='store_true',\n",
    "                            help='use custom param initialization for BERT')\n",
    "\n",
    "        # misc params\n",
    "        parser.add_argument('--activation-fn',\n",
    "                            choices=utils.get_available_activation_fns(),\n",
    "                            # \"relu\",\"gelu\",\"gelu_accurate\",\"tanh\",\"linear\"\n",
    "                            help='activation function to use')\n",
    "        parser.add_argument('--pooler-activation-fn',\n",
    "                            choices=utils.get_available_activation_fns(),\n",
    "                            help='Which activation function to use for pooler layer.')\n",
    "        parser.add_argument('--encoder-normalize-before', action='store_true',\n",
    "                            help='apply layernorm before each encoder block')\n",
    "\n",
    "        parser.add_argument('--pretrained-file', type=str, default=None,\n",
    "                            help='path to load the previous encoder')\n",
    "        parser.add_argument('--label-padding', type=int, default=-1,\n",
    "                            help='used for decided which token for padding labels.')\n",
    "\n",
    "    def forward(self, src_tokens, twod_tokens=None, is_twod=False, segment_labels=None, extra_only=False, masked_only=False,\n",
    "                classification_head_name=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Run the forward pass for a encoder-only model.\n",
    "\n",
    "        Feeds a batch of tokens through the encoder to generate features.\n",
    "\n",
    "        Args:\n",
    "            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n",
    "            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n",
    "\n",
    "        Returns:\n",
    "            the encoder's output, typically of shape `(batch, src_len, features)`\n",
    "        \"\"\"\n",
    "        x, last_attn_bias, extra = self.encoder(src_tokens, twod_tokens=twod_tokens, is_twod=is_twod, segment_labels=segment_labels,\n",
    "                                extra_only=False, masked_only=False, **kwargs)\n",
    "        \n",
    "        if classification_head_name is not None:\n",
    "            x = self.classification_heads[classification_head_name](x)\n",
    "        return x, last_attn_bias, extra\n",
    "\n",
    "    def get_normalized_probs(self, net_output, log_probs, sample=None):\n",
    "        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n",
    "        logits = net_output[0].float()\n",
    "        if log_probs:\n",
    "            return F.log_softmax(logits, dim=-1)\n",
    "        else:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum length supported by the model.\"\"\"\n",
    "        return self.encoder.max_positions\n",
    "\n",
    "    def register_classification_head(self, name, **kwargs):\n",
    "        \"\"\"Register a classification head.\"\"\"\n",
    "\n",
    "        if name in self.classification_heads:\n",
    "            logger.warning('Using registered head \"{}\"'.format(name))\n",
    "        else:\n",
    "            self.classification_heads[name] = prediction.DOWNSTREAM_HEADS[name](self.args)\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, args, task):\n",
    "        \"\"\"Build a new model instance.\n",
    "        This model is 1) pre-trained for a transformer-encoder\n",
    "        2) fine-tuning trained for various evaluation tasks include TAPE's 5 tasks\n",
    "        3) capable of embedding & validating & testing & evaluation\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(args, 'max_positions'):\n",
    "            args.max_positions = args.tokens_per_sample\n",
    "\n",
    "        if args.pretrained_file:\n",
    "            \"\"\"\n",
    "            Arguments:\n",
    "                state_dict (dict): a dict containing parameters and\n",
    "                    persistent buffers.\n",
    "                strict (bool, optional): whether to strictly enforce that the keys\n",
    "                    in :attr:`state_dict` match the keys returned by this module's\n",
    "                    :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "            make sure all arguments are present in older models\n",
    "            \"\"\"\n",
    "            models, pharsed_args, _ = checkpoint_utils.load_model_ensemble_and_task(\n",
    "                args.pretrained_file.split(os.pathsep),\n",
    "                arg_overrides={\n",
    "                    'data': getattr(args, \"data\", None), \n",
    "                    'eval_task': None,  # legacy\n",
    "                    'max_positions': args.max_positions, \n",
    "                    'tokens_per_sample': args.tokens_per_sample,\n",
    "                    'task': args.task,\n",
    "                    'arch': args.arch,\n",
    "                    'criterion': args.criterion,\n",
    "                    '_name': None, # For compatibility\n",
    "                    'eval_task': args.eval_task\n",
    "                },\n",
    "                suffix=getattr(args, \"checkpoint_suffix\", \"\"),\n",
    "                task=task,\n",
    "                strict=False,  # TODO\n",
    "            )\n",
    "            model = models[0]\n",
    "            model.encoder.remove_head()\n",
    "            print('Loaded pre-trained model encoder from ', args.pretrained_file.split(os.pathsep))\n",
    "            logger.info(args)\n",
    "            return cls(args, model.encoder)\n",
    "\n",
    "        base_architecture(args)\n",
    "\n",
    "        logger.info(args)\n",
    "\n",
    "        encoder = RNAMaskedLMEncoder(args, task.dictionary)\n",
    "\n",
    "        return cls(args, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define task\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from fairseq import metrics, search, tokenizer, utils\n",
    "from fairseq.data import (\n",
    "    data_utils,\n",
    "    Dictionary,\n",
    "    iterators,\n",
    "    FairseqDataset,\n",
    "    IdDataset,\n",
    "    NestedDictionaryDataset,\n",
    "    NumSamplesDataset,\n",
    "    NumelDataset,\n",
    "    PrependTokenDataset,\n",
    "    SortDataset,\n",
    "    TokenBlockDataset,\n",
    "    MaskTokensDataset,\n",
    "    PadDataset,\n",
    "    BaseWrapperDataset,\n",
    ")\n",
    "from fairseq.data.encoders.utils import get_whole_word_mask\n",
    "from fairseq.data.shorten_dataset import maybe_shorten_dataset\n",
    "from fairseq.tasks import FairseqTask, register_task\n",
    "class RNAMaskedLMTask(FairseqTask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dictionary (Dictionary): the dictionary for the input of the task\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        \"\"\"Add task-specific arguments to the parser.\"\"\"\n",
    "        parser.add_argument('data', help='colon separated path to data directories list, \\\n",
    "                            will be iterated upon during epochs in round-robin manner')\n",
    "        parser.add_argument('--sample-break-mode', default='complete',\n",
    "                            choices=['none', 'complete', 'complete_doc', 'eos'],\n",
    "                            help='If omitted or \"none\", fills each sample with tokens-per-sample '\n",
    "                                 'tokens. If set to \"complete\", splits samples only at the end '\n",
    "                                 'of sentence, but may include multiple sentences per sample. '\n",
    "                                 '\"complete_doc\" is similar but respects doc boundaries. '\n",
    "                                 'If set to \"eos\", includes only one sentence per sample.')\n",
    "        parser.add_argument('--tokens-per-sample', default=512, type=int,\n",
    "                            help='max number of total tokens over all segments '\n",
    "                                 'per sample for BERT dataset')\n",
    "        parser.add_argument('--mask-prob', default=0.15, type=float,\n",
    "                            help='probability of replacing a token with mask')\n",
    "        parser.add_argument('--leave-unmasked-prob', default=0.1, type=float,\n",
    "                            help='probability that a masked token is unmasked')\n",
    "        parser.add_argument('--random-token-prob', default=0.1, type=float,\n",
    "                            help='probability of replacing a token with a random token')\n",
    "        parser.add_argument('--freq-weighted-replacement', default=False, action='store_true',\n",
    "                            help='sample random replacement words based on word frequencies')\n",
    "        parser.add_argument('--mask-whole-words', default=False, action='store_true',\n",
    "                            help='mask whole words; you may also want to set --bpe')\n",
    "        parser.add_argument('--shorten-method', default='none',\n",
    "                            choices=['none', 'truncate', 'random_crop'],\n",
    "                            help='if not none, shorten sequences that exceed --tokens-per-sample')\n",
    "        parser.add_argument('--shorten-data-split-list', default='',\n",
    "                            help='comma-separated list of dataset splits to apply shortening to, '\n",
    "                                 'e.g., \"train,valid\" (default: all dataset splits)')\n",
    "\n",
    "        parser.add_argument('--init-token', type=int, default=None,\n",
    "                            help='add token at the beginning of each batch item')\n",
    "        parser.add_argument('--separator-token', type=int, default=None,\n",
    "                            help='add separator token between inputs')\n",
    "        parser.add_argument('--no-shuffle', action='store_true', default=False)\n",
    "        parser.add_argument('--add-prev-output-tokens', action='store_true', default=False,\n",
    "                            help='add prev_output_tokens to sample, used for encoder-decoder arch')\n",
    "\n",
    "    def __init__(self, args, dictionary):\n",
    "        super().__init__(args)\n",
    "        self.args = args\n",
    "        self.dictionary = dictionary\n",
    "        self.seed = args.seed\n",
    "        if not hasattr(args, 'max_positions'):\n",
    "            self._max_positions = (\n",
    "                args.max_source_positions,\n",
    "                args.max_target_positions,\n",
    "            )\n",
    "        else:\n",
    "            self._max_positions = args.max_positions\n",
    "        args.tokens_per_sample = self._max_positions\n",
    "        # add mask token\n",
    "        self.mask_idx = dictionary.add_symbol(\"<mask>\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dictionary(cls, args, filename, source=True):\n",
    "        \"\"\"Load the dictionary from the filename\n",
    "\n",
    "        Args:\n",
    "            filename (str): the filename\n",
    "        \"\"\"\n",
    "        dictionary = Dictionary.load(filename)\n",
    "        dictionary.add_symbol('<mask>')\n",
    "        return dictionary\n",
    "\n",
    "    @classmethod\n",
    "    def setup_task(cls, args, **kwargs):\n",
    "        paths = utils.split_paths(args.data)\n",
    "        assert len(paths) > 0\n",
    "        dictionary = Dictionary.load(os.path.join(paths[0], \"dict.txt\"))\n",
    "        logger.info(\"dictionary: {} types\".format(len(dictionary)))\n",
    "        return cls(args, dictionary)\n",
    "\n",
    "    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n",
    "        \"\"\"Load a given dataset split.\n",
    "\n",
    "        Args:\n",
    "            split (str): name of the split (e.g., train, valid, test)\n",
    "        \"\"\"\n",
    "\n",
    "        mask_whole_words = get_whole_word_mask(self.args, self.source_dictionary) \\\n",
    "            if self.args.mask_whole_words else None\n",
    "        paths = utils.split_paths(self.args.data)\n",
    "        assert len(paths) > 0\n",
    "        data_path = paths[(epoch - 1) % len(paths)]\n",
    "        split_path = os.path.join(data_path, split)\n",
    "\n",
    "        dataset = data_utils.load_indexed_dataset(\n",
    "            split_path,\n",
    "            self.source_dictionary,\n",
    "            self.args.dataset_impl,\n",
    "            combine=combine,\n",
    "        )\n",
    "        if dataset is None:\n",
    "            raise FileNotFoundError('Dataset not found: {} ({})'.format(split, split_path))\n",
    "\n",
    "        dataset = maybe_shorten_dataset(\n",
    "            dataset,\n",
    "            split,\n",
    "            self.args.shorten_data_split_list,\n",
    "            self.args.shorten_method,\n",
    "            self.args.tokens_per_sample,\n",
    "            self.args.seed,\n",
    "        )\n",
    "\n",
    "        # create continuous blocks of tokens\n",
    "        dataset = TokenBlockDataset(\n",
    "            dataset,\n",
    "            dataset.sizes,\n",
    "            self.args.tokens_per_sample - 1,  # one less for <s>\n",
    "            pad=self.source_dictionary.pad(),\n",
    "            eos=self.source_dictionary.eos(),\n",
    "            break_mode=self.args.sample_break_mode,\n",
    "        )\n",
    "        logger.info('loaded {} blocks from: {}'.format(len(dataset), split_path))\n",
    "\n",
    "        # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n",
    "        dataset = PrependTokenDataset(dataset, self.source_dictionary.bos())\n",
    "\n",
    "        src_dataset, tgt_dataset = MaskTokensDataset.apply_mask(\n",
    "            dataset,\n",
    "            self.source_dictionary,\n",
    "            pad_idx=self.source_dictionary.pad(),\n",
    "            mask_idx=self.mask_idx,\n",
    "            seed=self.args.seed,\n",
    "            mask_prob=self.args.mask_prob,\n",
    "            leave_unmasked_prob=self.args.leave_unmasked_prob,\n",
    "            random_token_prob=self.args.random_token_prob,\n",
    "            freq_weighted_replacement=self.args.freq_weighted_replacement,\n",
    "            mask_whole_words=mask_whole_words,\n",
    "        )\n",
    "\n",
    "        with data_utils.numpy_seed(self.args.seed + epoch):\n",
    "            shuffle = np.random.permutation(len(src_dataset))\n",
    "\n",
    "        netinput_dataset = {\n",
    "            'id': IdDataset(),\n",
    "            'net_input': {\n",
    "                'src_tokens': PadDataset(\n",
    "                    src_dataset,\n",
    "                    pad_idx=self.source_dictionary.pad(),\n",
    "                    left_pad=False,\n",
    "                ),\n",
    "                'src_lengths': NumelDataset(src_dataset, reduce=False),\n",
    "                # 'pair':,\n",
    "            },\n",
    "            'target': PadDataset(\n",
    "                tgt_dataset,\n",
    "                pad_idx=self.source_dictionary.pad(),\n",
    "                left_pad=False,\n",
    "            ),\n",
    "            'nsentences': NumSamplesDataset(),\n",
    "            'ntokens': NumelDataset(src_dataset, reduce=True),\n",
    "        }\n",
    "\n",
    "        self.datasets[split] = SortDataset(\n",
    "            NestedDictionaryDataset(\n",
    "                netinput_dataset,\n",
    "                sizes=[src_dataset.sizes],\n",
    "            ),\n",
    "            sort_order=[\n",
    "                shuffle,\n",
    "                src_dataset.sizes,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def build_model(self, args):\n",
    "        from fairseq import models\n",
    "        model = models.build_model(args, self)\n",
    "        return model\n",
    "\n",
    "    def get_batch_iterator(\n",
    "            self,\n",
    "            dataset,\n",
    "            max_tokens=None,\n",
    "            max_sentences=None,\n",
    "            max_positions=None,\n",
    "            ignore_invalid_inputs=False,\n",
    "            required_batch_size_multiple=1,\n",
    "            seed=1,\n",
    "            num_shards=1,\n",
    "            shard_id=0,\n",
    "            num_workers=0,\n",
    "            epoch=1,\n",
    "            data_buffer_size=0,\n",
    "            disable_iterator_cache=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get an iterator that yields batches of data from the given dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (~fairseq.data.FairseqDataset): dataset to batch\n",
    "            max_tokens (int, optional): max number of tokens in each batch\n",
    "                (default: None).\n",
    "            max_sentences (int, optional): max number of sentences in each\n",
    "                batch (default: None).\n",
    "            max_positions (optional): max sentence length supported by the\n",
    "                model (default: None).\n",
    "            ignore_invalid_inputs (bool, optional): don't raise Exception for\n",
    "                sentences that are too long (default: False).\n",
    "            required_batch_size_multiple (int, optional): require batch size to\n",
    "                be a multiple of N (default: 1).\n",
    "            seed (int, optional): seed for random number generator for\n",
    "                reproducibility (default: 1).\n",
    "            num_shards (int, optional): shard the data iterator into N\n",
    "                shards (default: 1).\n",
    "            shard_id (int, optional): which shard of the data iterator to\n",
    "                return (default: 0).\n",
    "            num_workers (int, optional): how many subprocesses to use for data\n",
    "                loading. 0 means the data will be loaded in the main process\n",
    "                (default: 0).\n",
    "            epoch (int, optional): the epoch to start the iterator from\n",
    "                (default: 1).\n",
    "            data_buffer_size (int, optional): number of batches to\n",
    "                preload (default: 0).\n",
    "            disable_iterator_cache (bool, optional): don't cache the\n",
    "                EpochBatchIterator (ignores `FairseqTask::can_reuse_epoch_itr`)\n",
    "                (default: False).\n",
    "        Returns:\n",
    "            ~fairseq.iterators.EpochBatchIterator: a batched iterator over the\n",
    "                given dataset split\n",
    "        \"\"\"\n",
    "        can_reuse_epoch_itr = not disable_iterator_cache and self.can_reuse_epoch_itr(\n",
    "            dataset\n",
    "        )\n",
    "        if can_reuse_epoch_itr and dataset in self.dataset_to_epoch_iter:\n",
    "            logger.debug(\"reusing EpochBatchIterator for epoch {}\".format(epoch))\n",
    "            return self.dataset_to_epoch_iter[dataset]\n",
    "\n",
    "        assert isinstance(dataset, FairseqDataset)\n",
    "\n",
    "        # initialize the dataset with the correct starting epoch\n",
    "        dataset.set_epoch(epoch)\n",
    "\n",
    "        # get indices ordered by example size\n",
    "        with data_utils.numpy_seed(seed):\n",
    "            indices = dataset.ordered_indices()\n",
    "\n",
    "        # filter examples that are too large\n",
    "        if max_positions is not None:\n",
    "            indices = self.filter_indices_by_size(\n",
    "                indices, dataset, max_positions, ignore_invalid_inputs\n",
    "            )\n",
    "\n",
    "        # create mini-batches with given size constraints\n",
    "        batch_sampler = dataset.batch_by_size(\n",
    "            indices,\n",
    "            max_tokens=max_tokens,\n",
    "            max_sentences=max_sentences,\n",
    "            required_batch_size_multiple=required_batch_size_multiple,\n",
    "        )\n",
    "\n",
    "        # return a reusable, sharded iterator\n",
    "        epoch_iter = iterators.EpochBatchIterator(\n",
    "            dataset=dataset,\n",
    "            collate_fn=dataset.collater,\n",
    "            batch_sampler=batch_sampler,\n",
    "            seed=seed,\n",
    "            num_shards=num_shards,\n",
    "            shard_id=shard_id,\n",
    "            num_workers=num_workers,\n",
    "            epoch=epoch,\n",
    "            buffer_size=data_buffer_size,\n",
    "        )\n",
    "\n",
    "        if can_reuse_epoch_itr:\n",
    "            self.dataset_to_epoch_iter[dataset] = epoch_iter\n",
    "\n",
    "        return epoch_iter\n",
    "\n",
    "    def filter_indices_by_size(\n",
    "            self, indices, dataset, max_positions=None, ignore_invalid_inputs=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter examples that are too large\n",
    "\n",
    "        Args:\n",
    "            indices (np.array): original array of sample indices\n",
    "            dataset (~fairseq.data.FairseqDataset): dataset to batch\n",
    "            max_positions (optional): max sentence length supported by the\n",
    "                model (default: None).\n",
    "            ignore_invalid_inputs (bool, optional): don't raise Exception for\n",
    "                sentences that are too long (default: False).\n",
    "        Returns:\n",
    "            np.array: array of filtered sample indices\n",
    "        \"\"\"\n",
    "        indices, ignored = dataset.filter_indices_by_size(indices, max_positions)\n",
    "        if len(ignored) > 0:\n",
    "            if not ignore_invalid_inputs:\n",
    "                raise Exception(\n",
    "                    (\n",
    "                        \"Size of sample #{} is invalid (={}) since max_positions={}, \"\n",
    "                        \"skip this example with --skip-invalid-size-inputs-valid-test\"\n",
    "                    ).format(ignored[0], dataset.size(ignored[0]), max_positions)\n",
    "                )\n",
    "            logger.warning(\n",
    "                (\n",
    "                    \"{} samples have invalid sizes and will be skipped, \"\n",
    "                    \"max_positions={}, first few sample ids={}\"\n",
    "                ).format(len(ignored), max_positions, ignored[:10])\n",
    "            )\n",
    "        return indices\n",
    "\n",
    "    def max_positions(self):\n",
    "        return self._max_positions\n",
    "\n",
    "    @property\n",
    "    def source_dictionary(self):\n",
    "        return self.dictionary\n",
    "\n",
    "    @property\n",
    "    def target_dictionary(self):\n",
    "        return self.dictionary\n",
    "\n",
    "    @property\n",
    "    def label_dictionary(self):\n",
    "        return self._label_dictionary\n",
    "\n",
    "    def build_criterion(self, args):\n",
    "        \"\"\"\n",
    "        Build the :class:`~fairseq.criterions.FairseqCriterion` instance for\n",
    "        this task.\n",
    "\n",
    "        Args:\n",
    "            args (argparse.Namespace): parsed command-line arguments\n",
    "\n",
    "        Returns:\n",
    "            a :class:`~fairseq.criterions.FairseqCriterion` instance\n",
    "        \"\"\"\n",
    "        from fairseq import criterions\n",
    "\n",
    "        return criterions.build_criterion(args, self)\n",
    "\n",
    "    def train_step(\n",
    "            self, sample, model, criterion, optimizer, update_num, ignore_grad=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Do forward and backward, and return the loss as computed by *criterion*\n",
    "        for the given *model* and *sample*.\n",
    "\n",
    "        Args:\n",
    "            sample (dict): the mini-batch. The format is defined by the\n",
    "                :class:`~fairseq.data.FairseqDataset`.\n",
    "            model (~fairseq.models.BaseFairseqModel): the model\n",
    "            criterion (~fairseq.criterions.FairseqCriterion): the criterion\n",
    "            optimizer (~fairseq.optim.FairseqOptimizer): the optimizer\n",
    "            update_num (int): the current update\n",
    "            ignore_grad (bool): multiply loss by 0 if this is set to True\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the loss\n",
    "                - the sample size, which is used as the denominator for the\n",
    "                  gradient\n",
    "                - logging outputs to display while training\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        model.set_num_updates(update_num)\n",
    "        with torch.autograd.profiler.record_function(\"forward\"):\n",
    "            loss, sample_size, logging_output = criterion(model, sample)\n",
    "        if ignore_grad:\n",
    "            loss *= 0\n",
    "        with torch.autograd.profiler.record_function(\"backward\"):\n",
    "            optimizer.backward(loss)\n",
    "        return loss, sample_size, logging_output\n",
    "\n",
    "    def valid_step(self, sample, model, criterion):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss, sample_size, logging_output = criterion(model, sample)\n",
    "        return loss, sample_size, logging_output\n",
    "\n",
    "    def inference_step(self, generator, models, sample, prefix_tokens=None):\n",
    "        with torch.no_grad():\n",
    "            return generator.generate(models, sample, prefix_tokens=prefix_tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def logging_outputs_can_be_summed(criterion) -> bool:\n",
    "        \"\"\"\n",
    "        Whether the logging outputs returned by `train_step` and `valid_step` can\n",
    "        be summed across workers prior to calling `aggregate_logging_outputs`.\n",
    "        Setting this to True will improves distributed training speed.\n",
    "        \"\"\"\n",
    "        return criterion.logging_outputs_can_be_summed()\n",
    "\n",
    "    def dataset(self, split):\n",
    "        \"\"\"\n",
    "        Return a loaded dataset split.\n",
    "\n",
    "        Args:\n",
    "            split (str): name of the split (e.g., train, valid, test)\n",
    "\n",
    "        Returns:\n",
    "            a :class:`~fairseq.data.FairseqDataset` corresponding to *split*\n",
    "        \"\"\"\n",
    "        from fairseq.data import FairseqDataset\n",
    "\n",
    "        if split not in self.datasets:\n",
    "            raise KeyError(\"Dataset not loaded: \" + split)\n",
    "        if not isinstance(self.datasets[split], FairseqDataset):\n",
    "            raise TypeError(\"Datasets are expected to be of type FairseqDataset\")\n",
    "        return self.datasets[split]\n",
    "\n",
    "    def begin_epoch(self, epoch, model):\n",
    "        \"\"\"Hook function called before the start of each epoch.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def build_dictionary(\n",
    "            cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8\n",
    "    ):\n",
    "        \"\"\"Build the dictionary\n",
    "\n",
    "        Args:\n",
    "            filenames (list): list of filenames\n",
    "            workers (int): number of concurrent workers\n",
    "            threshold (int): defines the minimum word count\n",
    "            nwords (int): defines the total number of words in the final dictionary,\n",
    "                including special symbols\n",
    "            padding_factor (int): can be used to pad the dictionary size to be a\n",
    "                multiple of 8, which is important on some hardware (e.g., Nvidia\n",
    "                Tensor Cores).\n",
    "        \"\"\"\n",
    "        d = Dictionary()\n",
    "        for filename in filenames:\n",
    "            Dictionary.add_file_to_dictionary(\n",
    "                filename, d, tokenizer.tokenize_line, workers\n",
    "            )  # TODO: Here the tokenizer is harmful\n",
    "        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n",
    "        return d\n",
    "\n",
    "    def build_generator(\n",
    "            self, models, args,\n",
    "            seq_gen_cls=None, extra_gen_cls_kwargs=None\n",
    "    ):\n",
    "        if getattr(args, \"score_reference\", False):\n",
    "            from fairseq.sequence_scorer import SequenceScorer\n",
    "\n",
    "            return SequenceScorer(\n",
    "                self.target_dictionary,\n",
    "                compute_alignment=getattr(args, \"print_alignment\", False),\n",
    "            )\n",
    "\n",
    "        from fairseq.sequence_generator import (\n",
    "            SequenceGenerator,\n",
    "            SequenceGeneratorWithAlignment,\n",
    "        )\n",
    "\n",
    "        # Choose search strategy. Defaults to Beam Search.\n",
    "        sampling = getattr(args, \"sampling\", False)\n",
    "        sampling_topk = getattr(args, \"sampling_topk\", -1)\n",
    "        sampling_topp = getattr(args, \"sampling_topp\", -1.0)\n",
    "        diverse_beam_groups = getattr(args, \"diverse_beam_groups\", -1)\n",
    "        diverse_beam_strength = getattr(args, \"diverse_beam_strength\", 0.5)\n",
    "        match_source_len = getattr(args, \"match_source_len\", False)\n",
    "        diversity_rate = getattr(args, \"diversity_rate\", -1)\n",
    "        if (\n",
    "                sum(\n",
    "                    int(cond)\n",
    "                    for cond in [\n",
    "                        sampling,\n",
    "                        diverse_beam_groups > 0,\n",
    "                        match_source_len,\n",
    "                        diversity_rate > 0,\n",
    "                    ]\n",
    "                )\n",
    "                > 1\n",
    "        ):\n",
    "            raise ValueError(\"Provided Search parameters are mutually exclusive.\")\n",
    "        assert sampling_topk < 0 or sampling, \"--sampling-topk requires --sampling\"\n",
    "        assert sampling_topp < 0 or sampling, \"--sampling-topp requires --sampling\"\n",
    "\n",
    "        if sampling:\n",
    "            search_strategy = search.Sampling(\n",
    "                self.target_dictionary, sampling_topk, sampling_topp\n",
    "            )\n",
    "        elif diverse_beam_groups > 0:\n",
    "            search_strategy = search.DiverseBeamSearch(\n",
    "                self.target_dictionary, diverse_beam_groups, diverse_beam_strength\n",
    "            )\n",
    "        elif match_source_len:\n",
    "            # this is useful for tagging applications where the output\n",
    "            # length should match the input length, so we hardcode the\n",
    "            # length constraints for simplicity\n",
    "            search_strategy = search.LengthConstrainedBeamSearch(\n",
    "                self.target_dictionary,\n",
    "                min_len_a=1,\n",
    "                min_len_b=0,\n",
    "                max_len_a=1,\n",
    "                max_len_b=0,\n",
    "            )\n",
    "        elif diversity_rate > -1:\n",
    "            search_strategy = search.DiverseSiblingsSearch(\n",
    "                self.target_dictionary, diversity_rate\n",
    "            )\n",
    "        else:\n",
    "            search_strategy = search.BeamSearch(self.target_dictionary)\n",
    "\n",
    "        if seq_gen_cls is None:\n",
    "            if getattr(args, \"print_alignment\", False):\n",
    "                seq_gen_cls = SequenceGeneratorWithAlignment\n",
    "            else:\n",
    "                seq_gen_cls = SequenceGenerator\n",
    "        extra_gen_cls_kwargs = extra_gen_cls_kwargs or {}\n",
    "        return seq_gen_cls(\n",
    "            models,\n",
    "            self.target_dictionary,\n",
    "            beam_size=getattr(args, \"beam\", 5),\n",
    "            max_len_a=getattr(args, \"max_len_a\", 0),\n",
    "            max_len_b=getattr(args, \"max_len_b\", 200),\n",
    "            min_len=getattr(args, \"min_len\", 1),\n",
    "            normalize_scores=(not getattr(args, \"unnormalized\", False)),\n",
    "            len_penalty=getattr(args, \"lenpen\", 1),\n",
    "            unk_penalty=getattr(args, \"unkpen\", 0),\n",
    "            temperature=getattr(args, \"temperature\", 1.0),\n",
    "            match_source_len=getattr(args, \"match_source_len\", False),\n",
    "            no_repeat_ngram_size=getattr(args, \"no_repeat_ngram_size\", 0),\n",
    "            search_strategy=search_strategy,\n",
    "            **extra_gen_cls_kwargs,\n",
    "        )\n",
    "\n",
    "    def reduce_metrics(self, logging_outputs, criterion):\n",
    "        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n",
    "        # backward compatibility for tasks that override aggregate_logging_outputs\n",
    "        base_func = FairseqTask.aggregate_logging_outputs\n",
    "        self_func = getattr(self, \"aggregate_logging_outputs\").__func__\n",
    "        if self_func is not base_func:\n",
    "            utils.deprecation_warning(\n",
    "                \"Tasks should implement the reduce_metrics API. \"\n",
    "                \"Falling back to deprecated aggregate_logging_outputs API.\"\n",
    "            )\n",
    "            agg_logging_outputs = self.aggregate_logging_outputs(\n",
    "                logging_outputs, criterion\n",
    "            )\n",
    "            for k, v in agg_logging_outputs.items():\n",
    "                metrics.log_scalar(k, v)\n",
    "            return\n",
    "\n",
    "        if not any(\"ntokens\" in log for log in logging_outputs):\n",
    "            warnings.warn(\n",
    "                \"ntokens not found in Criterion logging outputs, cannot log wpb or wps\"\n",
    "            )\n",
    "        else:\n",
    "            ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n",
    "            metrics.log_scalar(\"wpb\", ntokens, priority=180, round=1)\n",
    "            metrics.log_speed(\"wps\", ntokens, priority=90, round=1)\n",
    "\n",
    "        if not any(\"nsentences\" in log for log in logging_outputs):\n",
    "            warnings.warn(\n",
    "                \"nsentences not found in Criterion logging outputs, cannot log bsz\"\n",
    "            )\n",
    "        else:\n",
    "            nsentences = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n",
    "            metrics.log_scalar(\"bsz\", nsentences, priority=190, round=1)\n",
    "\n",
    "        criterion.__class__.reduce_metrics(logging_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = RNAMaskedLMTask.setup_task(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNABaseModel.build_model(args,task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_1d_input = torch.from_numpy(np.load('oned_input.npy'))\n",
    "fake_2d_input = torch.from_numpy(np.load('twod_input.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,last_attn_bias,_ = model(src_tokens=fake_1d_input, twod_tokens = fake_2d_input, is_twod = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,last_attn_bias,_ = model(src_tokens=fake_1d_input, twod_tokens = fake_2d_input, is_twod = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 3.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 5.104291696905211,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 6.460877141349489,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 4.698285847195373,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 4.213061319425266,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " 3.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " 0.0,\n",
       " 6.438659148273005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 3.9134483705564294,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 5.104291696905211,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 3.891230377479945,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.8195919791379005,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 5.104291696905211,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 5.126509689981695,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 3.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 3.891230377479945,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.213061319425266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 6.032653298563167,\n",
       " 0.0,\n",
       " 3.891230377479945,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.485224527770107,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.698285847195373,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_2d_input.reshape(-1).tolist()[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3582, -0.8060,  0.1415,  ...,    -inf,    -inf,    -inf],\n",
       "         [-1.0296, -1.0209, -1.0913,  ...,    -inf,    -inf,    -inf],\n",
       "         [-0.0540, -1.0102, -0.0391,  ...,    -inf,    -inf,    -inf],\n",
       "         ...,\n",
       "         [ 1.1059,  0.9740,  1.1689,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 1.0748,  0.9261,  1.0580,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.9975,  0.9247,  1.0629,  ...,    -inf,    -inf,    -inf]],\n",
       "\n",
       "        [[ 0.6971, -0.5894,  0.3211,  ...,    -inf,    -inf,    -inf],\n",
       "         [-0.4702, -0.8376, -0.8453,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.2412, -0.7107,  0.1192,  ...,    -inf,    -inf,    -inf],\n",
       "         ...,\n",
       "         [ 1.1387,  1.1225,  1.1855,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.9321,  0.9640,  1.0032,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 1.1462,  1.1105,  1.1241,  ...,    -inf,    -inf,    -inf]],\n",
       "\n",
       "        [[ 0.4980, -0.7285,  0.1258,  ..., -0.1916, -0.2861, -0.2255],\n",
       "         [-0.4476, -0.6622, -0.7877,  ..., -1.1961, -1.2165, -1.1390],\n",
       "         [ 0.5559, -0.6780,  0.3220,  ..., -0.0441,  0.0367,  0.1292],\n",
       "         ...,\n",
       "         [ 0.6463, -0.7297,  0.3138,  ...,  0.0939, -0.1496,  0.2062],\n",
       "         [ 0.4320, -0.6712,  0.1630,  ...,  0.3908,  0.2271,  0.4108],\n",
       "         [ 0.6676, -0.5674,  0.3357,  ...,  0.3277,  0.2815,  0.5969]],\n",
       "\n",
       "        [[ 0.1735,  0.3652, -0.7501,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.2367,  0.2747, -0.8149,  ...,    -inf,    -inf,    -inf],\n",
       "         [-1.0569, -0.9283, -0.9823,  ...,    -inf,    -inf,    -inf],\n",
       "         ...,\n",
       "         [ 0.6581,  1.0061,  1.1225,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.7622,  1.0355,  1.0707,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.7561,  1.1192,  1.1430,  ...,    -inf,    -inf,    -inf]],\n",
       "\n",
       "        [[-0.0186, -0.2012, -0.0746,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.1197,  0.0622,  1.4274,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.1005,  1.2523,  0.1448,  ...,    -inf,    -inf,    -inf],\n",
       "         ...,\n",
       "         [ 0.9640,  0.9454,  1.0339,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.9751,  1.1008,  1.0188,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.9187,  0.9844,  1.0526,  ...,    -inf,    -inf,    -inf]],\n",
       "\n",
       "        [[-0.0091,  0.2773, -0.9360,  ...,    -inf,    -inf,    -inf],\n",
       "         [-0.0655,  0.1329, -0.9465,  ...,    -inf,    -inf,    -inf],\n",
       "         [-0.9091, -0.7957, -0.8829,  ...,    -inf,    -inf,    -inf],\n",
       "         ...,\n",
       "         [ 0.7999,  0.8872,  1.1353,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.7918,  0.8369,  1.1420,  ...,    -inf,    -inf,    -inf],\n",
       "         [ 0.7235,  0.8639,  1.1454,  ...,    -inf,    -inf,    -inf]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dstask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5a3cfb7a02989fe63c4151ebb39fd8693dda42b9f0894cffc97d71da260a393"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
